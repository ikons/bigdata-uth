# Î•ÎºÏ„Î­Î»ÎµÏƒÎ· ÏƒÏ‡ÎµÏƒÎ¹Î±ÎºÏÎ½ ÎµÏÏ‰Ï„Î·Î¼Î¬Ï„Ï‰Î½ ÏƒÏ„Î¿ Spark Î¼Îµ Ï‡ÏÎ®ÏƒÎ· RDD ÎºÎ±Î¹ DataFrames

## Î ÏÎ¿ÎµÏ„Î¿Î¹Î¼Î±ÏƒÎ¯Î± Î”ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½

Î“Î¹Î± Î½Î± ÎºÎ±Ï„ÎµÎ²Î¬ÏƒÎ¿Ï…Î¼Îµ Ï„Î± Î±ÏÏ‡ÎµÎ¯Î± Ï€Î¿Ï… Î¸Î± Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î®ÏƒÎ¿Ï…Î¼Îµ, Î±ÏÏ‡Î¹ÎºÎ¬ Ï€ÏÎ­Ï€ÎµÎ¹ Î½Î± ÎºÎ±Ï„ÎµÎ²Î¬ÏƒÎ¿Ï…Î¼Îµ Ï„Î± **Ï€Î±ÏÎ±Î´ÎµÎ¯Î³Î¼Î±Ï„Î±** ÎºÎ±Î¹ Ï„Î¿Î½ **ÎºÏÎ´Î¹ÎºÎ±**. Î’ÎµÎ²Î±Î¹Ï‰Î¸ÎµÎ¯Ï„Îµ ÏŒÏ„Î¹ Î­Ï‡ÎµÏ„Îµ ÎµÎ³ÎºÎ±Ï„Î±ÏƒÏ„Î®ÏƒÎµÎ¹ Ï„Î¿ `git`.


```bash
cd ~
```


```bash
git clone https://github.com/ikons/bigdata-uth.git
```

Î‘Î½Î­Î²Î±ÏƒÎ¼Î± Ï„Ï‰Î½ Î±ÏÏ‡ÎµÎ¯Ï‰Î½ Ï„Ï‰Î½ Ï€Î±ÏÎ±Î´ÎµÎ¹Î³Î¼Î¬Ï„Ï‰Î½ ÏƒÏ„Î¿ HDFS

Î£Ï„Î· ÏƒÏ…Î½Î­Ï‡ÎµÎ¹Î±, Î±Î½ÎµÎ²Î¬Î¶Î¿Ï…Î¼Îµ Î¿Î»ÏŒÎºÎ»Î·ÏÎ¿ Ï„Î¿Î½ Ï†Î¬ÎºÎµÎ»Î¿ `examples` ÏƒÏ„Î¿Î½ ÎºÎ±Ï„Î¬Î»Î¿Î³Î¿ Ï„Î¿Ï… HDFS: `/user/<ÏŒÎ½Î¿Î¼Î±_Ï‡ÏÎ®ÏƒÏ„Î·>/examples`

```bash
cd bigdata-uth
```

```bash
hadoop fs -put examples examples
```

Î¤ÏÏÎ± Î¼Ï€Î¿ÏÎ¿ÏÎ¼Îµ Î½Î± Î²ÎµÎ²Î±Î¹Ï‰Î¸Î¿ÏÎ¼Îµ ÏŒÏ„Î¹ Ï„Î± Î±ÏÏ‡ÎµÎ¯Î± Î±Î½Î­Î²Î·ÎºÎ±Î½ ÏƒÏ‰ÏƒÏ„Î¬, ÎµÎºÏ„ÎµÎ»ÏÎ½Ï„Î±Ï‚:

```bash
hadoop fs -ls examples
```

Î‘Î½Î­Î²Î±ÏƒÎ¼Î± Ï„Ï‰Î½ Î±ÏÏ‡ÎµÎ¯Ï‰Î½ ÎºÏÎ´Î¹ÎºÎ± ÏƒÏ„Î¿ HDFS




**Î‘Î½Ï„Î¹ÎºÎ±Ï„Î¬ÏƒÏ„Î±ÏƒÎ· Ï„Î¿Ï… username Ï€ÏÎ¹Î½ Ï„Î·Î½ Î±Ï€Î¿ÏƒÏ„Î¿Î»Î® ÏƒÏ„Î¿ HDFS**: Î ÏÎ¹Î½ Î±Î½ÎµÎ²Î¬ÏƒÎµÏ„Îµ Ï„Î¿Î½ Ï†Î¬ÎºÎµÎ»Î¿ `code` ÏƒÏ„Î¿ HDFS, ÎµÎºÏ„ÎµÎ»Î­ÏƒÏ„Îµ Ï„Î·Î½ Ï€Î±ÏÎ±ÎºÎ¬Ï„Ï‰ ÎµÎ½Ï„Î¿Î»Î® Î³Î¹Î± Î½Î± Î±Î½Ï„Î¹ÎºÎ±Ï„Î±ÏƒÏ„Î®ÏƒÎµÏ„Îµ Ï„Î¿ username `ikons` (Ï€Î¿Ï… Ï…Ï€Î¬ÏÏ‡ÎµÎ¹ "ÎºÎ±ÏÏ†Ï‰Ï„Î¬" ÏƒÏ„Î± Ï€ÎµÏÎ¹ÏƒÏƒÏŒÏ„ÎµÏÎ± Î±ÏÏ‡ÎµÎ¯Î±) Î¼Îµ Ï„Î¿ Î´Î¹ÎºÏŒ ÏƒÎ±Ï‚ ÏŒÎ½Î¿Î¼Î± Ï‡ÏÎ®ÏƒÏ„Î· (Ï€.Ï‡. `student1`):

```bash
find code -type f -exec sed -i 's/ikons/<Ï„Î¿_ÏŒÎ½Î¿Î¼Î±_Ï‡ÏÎ®ÏƒÏ„Î·_ÏƒÎ±Ï‚>/g' {} +
```

ğŸ“Œ Î Î±ÏÎ¬Î´ÎµÎ¹Î³Î¼Î±:

```bash
find code -type f -exec sed -i 's/ikons/student1/g' {} +
```

Î‘Ï†Î¿Ï ÎºÎ¬Î½ÎµÏ„Îµ Ï„Î·Î½ Î±Î»Î»Î±Î³Î®, ÏƒÏ…Î½ÎµÏ‡Î¯ÏƒÏ„Îµ Î¼Îµ Ï„Î·Î½ Î±Ï€Î¿ÏƒÏ„Î¿Î»Î® ÏƒÏ„Î¿ HDFS:

```bash
hadoop fs -put code code
```


ğŸ’¡ **Tip**: ÎœÏ€Î¿ÏÎµÎ¯Ï„Îµ Î½Î± ÎµÏ€Î¹Î²ÎµÎ²Î±Î¹ÏÏƒÎµÏ„Îµ ÏŒÏ„Î¹ Î· Î±Î»Î»Î±Î³Î® Î­Î³Î¹Î½Îµ ÏƒÏ‰ÏƒÏ„Î¬ Î¼Îµ:

```bash
grep -rn "student1" code/
```

Î’ÎµÎ²Î±Î¹Ï‰Î¸ÎµÎ¯Ï„Îµ ÏŒÏ„Î¹ ÏŒÎ»Î± Ï„Î± Î±ÏÏ‡ÎµÎ¯Î± Î­Ï‡Î¿Ï…Î½ Î±Î½Î­Î²ÎµÎ¹ ÏƒÏ„Î¿ HDFS:


```bash
hadoop fs -ls code
```
## Map/Reduce


![Î•Î¹ÎºÏŒÎ½Î± 1](images/img1.png)

![Î•Î¹ÎºÏŒÎ½Î± 2](images/img2.png)

ÎˆÎ½Î± **Map/Reduce job** Ï€ÎµÏÎ¹Î»Î±Î¼Î²Î¬Î½ÎµÎ¹ Î­Î½Î± **ÏƒÏ„Î¬Î´Î¹Î¿ map** ÎºÎ±Î¹ Î­Î½Î± **ÏƒÏ„Î¬Î´Î¹Î¿ reduce**. Î£Ï„Î¿ ÏƒÏ„Î¬Î´Î¹Î¿ Ï„Î¿Ï… map, Î¿Î¹ **ÎºÏŒÎ¼Î²Î¿Î¹ ÎµÏÎ³Î¬Ï„ÎµÏ‚ (worker nodes)** ÏƒÏ„Î¿Ï…Ï‚ Î¿Ï€Î¿Î¯Î¿Ï…Ï‚ Î­Ï‡Î¿Ï…Î½ Î±Î½Î±Ï„ÎµÎ¸ÎµÎ¯ Ï„Î± map jobs ÎµÎºÏ„ÎµÎ»Î¿ÏÎ½ ÎµÏÎ³Î±ÏƒÎ¯Î± Ï€Î¬Î½Ï‰ ÏƒÎµ Î­Î½Î± **Ï„Î¼Î®Î¼Î± Ï„Ï‰Î½ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½** Ï€Î¿Ï… Ï„Î¿Ï…Ï‚ Î­Ï‡ÎµÎ¹ Î±Î½Î±Ï„ÎµÎ¸ÎµÎ¯ Î±Ï€ÏŒ Ï„Î¿Î½ **ÎºÏÏÎ¹Î¿ ÎºÏŒÎ¼Î²Î¿ (master VM)**. ÎŸ master Ï€ÏÎ¿Ï„Î¹Î¼Î¬ Î½Î± Î±Î½Î±Î¸Î­Ï„ÎµÎ¹ Î¿ÏÎ¹ÏƒÎ¼Î­Î½ÎµÏ‚ ÎµÏÎ³Î±ÏƒÎ¯ÎµÏ‚ ÏƒÎµ ÏƒÏ…Î³ÎºÎµÎºÏÎ¹Î¼Î­Î½Î¿Ï…Ï‚ ÎµÏÎ³Î¬Ï„ÎµÏ‚ Ï€Î¿Ï… **Î­Ï‡Î¿Ï…Î½ Î®Î´Î· Ï„Î¿ Î±Î½Ï„Î¯ÏƒÏ„Î¿Î¹Ï‡Î¿ ÎºÎ¿Î¼Î¼Î¬Ï„Î¹ Ï„Ï‰Î½ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ Ï„Î¿Ï€Î¹ÎºÎ¬**, ÏÏƒÏ„Îµ Î½Î± **ÎµÎ»Î±Ï‡Î¹ÏƒÏ„Î¿Ï€Î¿Î¹Î·Î¸ÎµÎ¯ Î· Ï‡ÏÎ®ÏƒÎ· Ï„Î¿Ï… Î´Î¹ÎºÏ„ÏÎ¿Ï…** Ï„Î·Ï‚ ÏƒÏ…ÏƒÏ„Î¿Î¹Ï‡Î¯Î±Ï‚ (Î±ÏÏ‡Î® **Ï„Î¿Ï€Î¹ÎºÏŒÏ„Î·Ï„Î±Ï‚ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ - data locality**). ÎœÎµÏ„Î¬ Ï„Î·Î½ ÎµÎºÏ„Î­Î»ÎµÏƒÎ· Ï„Î¿Ï… ÏƒÏ„Î±Î´Î¯Î¿Ï… map, Î¿Î¹ mappers ÏƒÏ„Î­Î»Î½Î¿Ï…Î½ Ï„Î± Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î± ÏƒÏ„Î¿Ï…Ï‚ reducers. ÎŸ master ÎºÎ±Î¸Î¿ÏÎ¯Î¶ÎµÎ¹ **ÏƒÎµ Ï€Î¿Î¹Î¿Î½ reducer** Ï€ÏÎ­Ï€ÎµÎ¹ Î½Î± ÏƒÏ„ÎµÎ¯Î»ÎµÎ¹ Ï„Î± Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î¬ Ï„Î¿Ï… ÎºÎ¬Î¸Îµ mapper â€“ ÎºÎ±Î¹ ÏƒÏ„Î­Î»Î½ÎµÎ¹ **Ï„Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î± Î¼Îµ Ï„Î¿ Î¯Î´Î¹Î¿ key ÏƒÏ„Î¿Î½ Î¯Î´Î¹Î¿ reducer**. ÎŸ ÏÏŒÎ»Î¿Ï‚ Ï„Î¿Ï… reducer ÎµÎ¯Î½Î±Î¹ Î½Î± **ÏƒÏ…Î½Î´Ï…Î¬ÏƒÎµÎ¹ Ï„Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î±** Î±Ï€ÏŒ Î´Î¹Î¬Ï†Î¿ÏÎ¿Ï…Ï‚ mappers Î³Î¹Î± Î½Î± Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î®ÏƒÎµÎ¹ Ï„Î·Î½ Ï„ÎµÎ»Î¹ÎºÎ® Î­Î¾Î¿Î´Î¿. (Î•Î½Î´Î¹Î¬Î¼ÎµÏƒÎ± Î²Î®Î¼Î±Ï„Î± Î¼ÎµÏ„Î±Î¾Ï Ï„Ï‰Î½ map ÎºÎ±Î¹ reduce jobs Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± ÎµÎ¯Î½Î±Î¹ Î±Ï€Î±ÏÎ±Î¯Ï„Î·Ï„Î±, Ï€.Ï‡. Ï„Î±Î¾Î¹Î½ÏŒÎ¼Î·ÏƒÎ·)

> Î‘Î¡Î§Î™ÎšÎ‘ Î”Î•Î”ÎŸÎœÎ•ÎÎ‘ -> MASTER -> Î‘ÎÎ‘Î˜Î•Î¤Î•Î™ MAP JOBS Î£Î• Î•Î¡Î“Î‘Î¤Î•Î£
>
> Î•Î¡Î“Î‘Î¤Î•Î£ Î•ÎšÎ¤Î•Î›ÎŸÎ¥Î Î¤Î‘ MAP JOBS ÎšÎ‘Î™ Î Î‘Î¡Î‘Î“ÎŸÎ¥Î Î–Î•Î¥Î“Î— (key, value)
>
> MAP 1:  [(1, DATA), (1 , DATA ), (1, DATA)] 
>
> MAP 2:  [(1, DATA), (1, DATA), (1, DATA)] 
>
> MAP 3:  [(2, DATA), (2, DATA)] 
>
> --------------------------------------------------------------------
>
> REDUCERS Î£Î¥ÎÎ”Î¥Î‘Î–ÎŸÎ¥Î Î¤Î‘ Î”Î•Î”ÎŸÎœÎ•ÎÎ‘ Î‘Î ÎŸ Î¤ÎŸÎ¥Î£ MAPPERS Î“Î™Î‘ ÎÎ‘ Î Î‘Î¡Î‘ÎÎŸÎ¥Î Î¤Î—Î Î¤Î•Î›Î™ÎšÎ— Î•ÎÎŸÎ”ÎŸ
>
> MAP1, MAP2 -> REDUCER1  
>
> MAP3 -> REDUCER2  
>
> REDUCER1, REDUCER2 -> FINAL_REDUCER -> Î¤Î•Î›Î™ÎšÎŸ OUTPUT

ÎˆÎ½Î± Î±Ï€ÏŒ Ï„Î± Ï€Î¹Î¿ ÎºÎ»Î±ÏƒÎ¹ÎºÎ¬ Ï€Î±ÏÎ±Î´ÎµÎ¯Î³Î¼Î±Ï„Î± Map/Reduce ÎµÎ¯Î½Î±Î¹ Ï„Î¿ Ï€ÏÏŒÎ²Î»Î·Î¼Î± ÎºÎ±Ï„Î±Î¼Î­Ï„ÏÎ·ÏƒÎ·Ï‚ Î»Î­Î¾ÎµÏ‰Î½ (word count). Î£Îµ Î±Ï…Ï„ÏŒ Ï„Î¿ Ï€Î±ÏÎ¬Î´ÎµÎ¹Î³Î¼Î±, Î­Ï‡Î¿Ï…Î¼Îµ Î­Î½Î± Î® Ï€ÎµÏÎ¹ÏƒÏƒÏŒÏ„ÎµÏÎ± Î­Î³Î³ÏÎ±Ï†Î± ÎºÎ±Î¹ Î¸Î­Î»Î¿Ï…Î¼Îµ Î½Î± Î²ÏÎ¿ÏÎ¼Îµ Ï€ÏŒÏƒÎµÏ‚ Ï†Î¿ÏÎ­Ï‚ ÎµÎ¼Ï†Î±Î½Î¯Î¶ÎµÏ„Î±Î¹ ÎºÎ¬Î¸Îµ Î»Î­Î¾Î· Î¼Î­ÏƒÎ± ÏƒÏ„Î¿ Î­Î³Î³ÏÎ±Ï†Î¿

Î“Î¹Î± Î½Î± ÎµÎºÏ„ÎµÎ»Î­ÏƒÎµÎ¹Ï‚ Ï„Î¿ Ï€ÏÏŒÎ³ÏÎ±Î¼Î¼Î± **wordcount.py**, Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¯Î·ÏƒÎµ Ï„Î·Î½ ÎµÎ¾Î®Ï‚ ÎµÎ½Ï„Î¿Î»Î®:

```bash
# âš ï¸ Î‘Î½Ï„Î¹ÎºÎ±Ï„Î­ÏƒÏ„Î·ÏƒÎµ Ï„Î¿ "ikons" Î¼Îµ Ï„Î¿ Î´Î¹ÎºÏŒ ÏƒÎ¿Ï… ğŸ‘‡ username
spark-submit hdfs://hdfs-namenode:9000/user/ikons/code/wordcount.py
```

ÎœÎµÏ„Î¬ Ï„Î·Î½ ÎµÎºÏ„Î­Î»ÎµÏƒÎ·, **Ï„ÏÎ­Î¾Îµ** k9s ÎºÎ±Î¹ Ï€Î±ÏÎ±ÎºÎ¿Î»Î¿ÏÎ¸Î·ÏƒÎµ Ï„Î¿ job Ï€Î¿Ï… ÎµÎºÏ„ÎµÎ»ÎµÎ¯Ï„Î±Î¹ ÏƒÏ„Î¿ **k8s (Kubernetes)**.

Wordcount.py:

```python
from pyspark.sql import SparkSession
# âš ï¸ Î‘Î½Ï„Î¹ÎºÎ±Ï„Î­ÏƒÏ„Î·ÏƒÎµ ğŸ‘‡ Ï„Î¿ "ikons" Î¼Îµ Ï„Î¿ Î´Î¹ÎºÏŒ ÏƒÎ¿Ï… username
username = "ikons"
sc = SparkSession \
    .builder \
    .appName("wordcount example") \
    .getOrCreate() \
    .sparkContext

# Î•Î›Î‘Î§Î™Î£Î¤ÎŸÎ ÎŸÎ™Î—Î£Î— Î•ÎÎŸÎ”Î©Î ÎšÎ‘Î¤Î‘Î“Î¡Î‘Î¦Î—Î£ (LOGGING)
sc.setLogLevel("ERROR")

# Î›Î®ÏˆÎ· Ï„Î¿Ï… job ID ÎºÎ±Î¹ ÎºÎ±Î¸Î¿ÏÎ¹ÏƒÎ¼ÏŒÏ‚ Î´Î¹Î±Î´ÏÎ¿Î¼Î®Ï‚ ÎµÎ¾ÏŒÎ´Î¿Ï…
job_id = sc.applicationId
output_dir = f"hdfs://hdfs-namenode:9000/user/{username}/wordcount_output_{job_id}"

# Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Î±ÏÏ‡ÎµÎ¯Î¿Ï… ÎºÎµÎ¹Î¼Î­Î½Î¿Ï… Î±Ï€ÏŒ Ï„Î¿ HDFS ÎºÎ±Î¹ Ï…Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ ÏƒÏ…Ï‡Î½Î¿Ï„Î®Ï„Ï‰Î½ Î»Î­Î¾ÎµÏ‰Î½
wordcount = (
    sc.textFile(f"hdfs://hdfs-namenode:9000/user/{username}/examples/text.txt") \
    .flatMap(lambda x: x.split(" "))       # Î”Î¹Î¬ÏƒÏ€Î±ÏƒÎ· ÎºÎ¬Î¸Îµ Î³ÏÎ±Î¼Î¼Î®Ï‚ ÏƒÎµ Î»Î­Î¾ÎµÎ¹Ï‚
    .map(lambda x: (x, 1))       # Î§Î±ÏÏ„Î¿Î³ÏÎ¬Ï†Î·ÏƒÎ· (map) ÎºÎ¬Î¸Îµ Î»Î­Î¾Î·Ï‚ ÏƒÎµ (Î»Î­Î¾Î·, 1)
    .reduceByKey(lambda x, y: x + y)     # Î†Î¸ÏÎ¿Î¹ÏƒÎ· ÎµÎ¼Ï†Î±Î½Î¯ÏƒÎµÏ‰Î½ Î³Î¹Î± ÎºÎ¬Î¸Îµ Î»Î­Î¾Î·
    .sortBy(lambda x: x[1], ascending=False) # Î¤Î±Î¾Î¹Î½ÏŒÎ¼Î·ÏƒÎ· ÎºÎ±Ï„Î¬ Ï†Î¸Î¯Î½Î¿Ï…ÏƒÎ± ÏƒÏ…Ï‡Î½ÏŒÏ„Î·Ï„Î±
)

# Î•Î¼Ï†Î¬Î½Î¹ÏƒÎ· Ï„Ï‰Î½ Î±Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î¬Ï„Ï‰Î½ (Î³Î¹Î± Î­Î»ÎµÎ³Ï‡Î¿)
for item in wordcount.coalesce(1).collect():
    print(item)

# Î£Ï…Î³Ï‡ÏÎ½ÎµÏ…ÏƒÎ· Î³Î¹Î± Î¼ÎµÎ¯Ï‰ÏƒÎ· Ï„Ï‰Î½ Î±ÏÏ‡ÎµÎ¯Ï‰Î½ ÎµÎ¾ÏŒÎ´Î¿Ï… ÎºÎ±Î¹ Î±Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· ÏƒÏ„Î¿ HDFS
wordcount.saveAsTextFile(output_dir)

# Î Î±ÏÎ¬Î´ÎµÎ¹Î³Î¼Î± Î±Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î¬Ï„Ï‰Î½:
# [('text', 3), ('this', 2), ('is', 2), ('like', 2), ('a', 2),
#  ('file', 2), ('words', 2), (',', 2), ('an', 1), ('of', 1),
#  ('with', 1), ('random', 1), ('example', 1)]
```

**Î Î¡ÎŸÎ•Î™Î”ÎŸÎ ÎŸÎ™Î—Î£Î—:** Î— Ï‡ÏÎ®ÏƒÎ· Ï„Î·Ï‚ ÏƒÏ…Î½Î¬ÏÏ„Î·ÏƒÎ·Ï‚ `collect` ÏƒÎµ RDDs Ï€Î¿Ï… Ï€ÎµÏÎ¹Î­Ï‡Î¿Ï…Î½ Î¼ÎµÎ³Î¬Î»Î¿ ÏŒÎ³ÎºÎ¿ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± Î¿Î´Î·Î³Î®ÏƒÎµÎ¹ ÏƒÎµ ÏƒÏ†Î¬Î»Î¼Î±. ÎœÏ€Î¿ÏÎµÎ¯Ï„Îµ Î½Î± Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î®ÏƒÎµÏ„Îµ Î¬Î»Î»ÎµÏ‚ ÏƒÏ…Î½Î±ÏÏ„Î®ÏƒÎµÎ¹Ï‚ Î³Î¹Î± Î½Î± ÏÎ¯Î¾ÎµÏ„Îµ Î¼Î¹Î± Î¼Î±Ï„Î¹Î¬ ÏƒÏ„Î¿ Ï„Î¹ Ï€ÎµÏÎ¹Î­Ï‡ÎµÎ¹ Ï„Î¿ RDD, ÏŒÏ€Ï‰Ï‚ Ï€.Ï‡. Î· `take(n)` â€“ ÏŒÏ€Î¿Ï… `n` ÎµÎ¯Î½Î±Î¹ Î¿ Î±ÏÎ¹Î¸Î¼ÏŒÏ‚ Ï„Ï‰Î½ ÎµÎ³Î³ÏÎ±Ï†ÏÎ½ Ï€Î¿Ï… Î¸Î­Î»Î¿Ï…Î¼Îµ Î½Î± Î±Î½Î±ÎºÏ„Î®ÏƒÎ¿Ï…Î¼Îµ.

**Î•Ï€ÎµÎ¾Î®Î³Î·ÏƒÎ· ÎšÏÎ´Î¹ÎºÎ±:**

Î‘ÏÏ‡Î¹ÎºÎ¬, Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î¿ÏÎ¼Îµ Î­Î½Î± **SparkSession** ÎºÎ±Î¹ Î­Î½Î± **SparkContext**. Î¤Î¿ `SparkSession` ÎµÎ¯Î½Î±Î¹ Ï„Î¿ ÏƒÎ·Î¼ÎµÎ¯Î¿ ÎµÎ¹ÏƒÏŒÎ´Î¿Ï… Î³Î¹Î± ÎºÎ¬Î¸Îµ Ï€ÏÎ¿Î³ÏÎ±Î¼Î¼Î±Ï„Î¹ÏƒÏ„Î¹ÎºÎ® Î²Î¹Î²Î»Î¹Î¿Î¸Î®ÎºÎ· ÏƒÏ„Î¿ Spark ÎºÎ±Î¹ ÎµÎ¯Î½Î±Î¹ Î±Ï€Î±ÏÎ±Î¯Ï„Î·Ï„Î¿ Î³Î¹Î± Î½Î± ÎµÎºÏ„ÎµÎ»ÎµÏƒÏ„ÎµÎ¯ Î¿Ï€Î¿Î¹Î¿ÏƒÎ´Î®Ï€Î¿Ï„Îµ ÎºÏÎ´Î¹ÎºÎ±Ï‚. Î¤Î¿ SparkContext ÎµÎ¯Î½Î±Î¹ Ï„Î¿ Î±Î½Ï„Î¯ÏƒÏ„Î¿Î¹Ï‡Î¿ ÏƒÎ·Î¼ÎµÎ¯Î¿ ÎµÎ¹ÏƒÏŒÎ´Î¿Ï… **ÎµÎ¹Î´Î¹ÎºÎ¬ Î³Î¹Î± RDDs**. Î£Ï„Î· ÏƒÏ…Î½Î­Ï‡ÎµÎ¹Î±, Ï„Î¿ Ï€ÏÏŒÎ³ÏÎ±Î¼Î¼Î± Î´Î¹Î±Î²Î¬Î¶ÎµÎ¹ Ï„Î¿ Î±ÏÏ‡ÎµÎ¯Î¿ `text.txt` Î±Ï€ÏŒ Ï„Î¿ HDFS ÎºÎ±Î¹ Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯ Î¼Î¯Î± **ÏƒÏ…Î½Î¬ÏÏ„Î·ÏƒÎ· lambda** Î³Î¹Î± Î½Î± Î´Î¹Î±Ï‡Ï‰ÏÎ¯ÏƒÎµÎ¹ Ï„Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î± ÎºÎ¬Î¸Îµ Ï†Î¿ÏÎ¬ Ï€Î¿Ï… ÎµÎ½Ï„Î¿Ï€Î¯Î¶ÎµÎ¹ **ÎºÎµÎ½ÏŒ (whitespace)**. ÎœÎ¹Î± **ÏƒÏ…Î½Î¬ÏÏ„Î·ÏƒÎ· lambda** ÎµÎ¯Î½Î±Î¹ Î¿Ï…ÏƒÎ¹Î±ÏƒÏ„Î¹ÎºÎ¬ Î¼Î¹Î± **Î±Î½ÏÎ½Ï…Î¼Î· ÏƒÏ…Î½Î¬ÏÏ„Î·ÏƒÎ·** Ï„Î·Î½ Î¿Ï€Î¿Î¯Î± Î¼Ï€Î¿ÏÎ¿ÏÎ¼Îµ Î½Î± Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î®ÏƒÎ¿Ï…Î¼Îµ Î³ÏÎ®Î³Î¿ÏÎ±, Ï‡Ï‰ÏÎ¯Ï‚ Î½Î± Ï‡ÏÎµÎ¹Î¬Î¶ÎµÏ„Î±Î¹ Î½Î± Ï„Î·Î½ Î¿ÏÎ¯ÏƒÎ¿Ï…Î¼Îµ Î¼Îµ ÏŒÎ½Î¿Î¼Î±. Î— ÏƒÏ…Î½Î¬ÏÏ„Î·ÏƒÎ· lambda Ï€Î¿Ï… Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯ Ï„Î¿ Ï€ÏÏŒÎ³ÏÎ±Î¼Î¼Î± Ï‰Ï‚ ÏŒÏÎ¹ÏƒÎ¼Î± Ï„Î·Ï‚ `flatMap` ÎµÎ¯Î½Î±Î¹:

```python
lambda x: x.split(" ")
```

Î“Î¹Î± Î¼Î¹Î± ÎµÎ¯ÏƒÎ¿Î´Î¿ `x`, ÎµÏ€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹ Î¼Î¯Î± Î»Î¯ÏƒÏ„Î± Î±Ï€ÏŒ Î»Î­Î¾ÎµÎ¹Ï‚ (Î´Î¹Î±Î¹ÏÏÎ½Ï„Î±Ï‚ Ï„Î¿ ÎºÎµÎ¯Î¼ÎµÎ½Î¿ ÎºÎ¬Î¸Îµ Ï†Î¿ÏÎ¬ Ï€Î¿Ï… Ï…Ï€Î¬ÏÏ‡ÎµÎ¹ ÎºÎµÎ½ÏŒ Î´Î¹Î¬ÏƒÏ„Î·Î¼Î±).

Î— Î²Î±ÏƒÎ¹ÎºÎ® Î´Î¹Î±Ï†Î¿ÏÎ¬ Ï„Î·Ï‚ `flatMap` ÏƒÎµ ÏƒÏ‡Î­ÏƒÎ· Î¼Îµ Ï„Î·Î½ `map` ÎµÎ¯Î½Î±Î¹ ÏŒÏ„Î¹ Î· **map ÎµÏ€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹ Ï€Î¿Î»Î»Î­Ï‚ Î»Î¯ÏƒÏ„ÎµÏ‚ (Î¼Î¯Î± Î³Î¹Î± ÎºÎ¬Î¸Îµ ÎµÎ¯ÏƒÎ¿Î´Î¿)**, ÎµÎ½Ï Î· `flatMap` **ÎµÎ½ÏÎ½ÎµÎ¹ ÏŒÎ»Î± Ï„Î± Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î± ÏƒÎµ Î¼Î¯Î± ÎµÎ½Î¹Î±Î¯Î± Î»Î¯ÏƒÏ„Î±**.

Î£Ï„Î· ÏƒÏ…Î½Î­Ï‡ÎµÎ¹Î±, Î¼Îµ Ï„Î· Ï‡ÏÎ®ÏƒÎ· Ï„Î·Ï‚ `map` Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î¿ÏÎ¼Îµ Î³Î¹Î± ÎºÎ¬Î¸Îµ Î»Î­Î¾Î· ÏƒÏ„Î¿ Î±ÏÏ‡ÎµÎ¯Î¿ Î­Î½Î± **Î¶ÎµÏÎ³Î¿Ï‚ (key, value)**.
Î˜Î± Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î®ÏƒÎ¿Ï…Î¼Îµ Î±Ï…Ï„ÏŒ Ï„Î¿ Î¶ÎµÏÎ³Î¿Ï‚ ÏƒÏ„Î¿ ÏƒÏ„Î¬Î´Î¹Î¿ Ï„Î·Ï‚ **ÏƒÏÎ½Î¿ÏˆÎ·Ï‚ (reduce)**:

- **Key = Î· Î»Î­Î¾Î·**
- **Value = 1**, Ï€Î¿Ï… Î±Î½Ï„Î¹Ï€ÏÎ¿ÏƒÏ‰Ï€ÎµÏÎµÎ¹ Î¼Î¯Î± ÎµÎ¼Ï†Î¬Î½Î¹ÏƒÎ· Ï„Î·Ï‚ Î»Î­Î¾Î·Ï‚

ÎˆÏ€ÎµÎ¹Ï„Î±, Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ Ï„Î· ÏƒÏ…Î½Î¬ÏÏ„Î·ÏƒÎ· `reduceByKey`. Î‘Ï…Ï„ÏŒ ÏƒÎ·Î¼Î±Î¯Î½ÎµÎ¹ ÏŒÏ„Î¹ ÏŒÎ»Î± Ï„Î± Î¶ÎµÏÎ³Î· Î¼Îµ Ï„Î¿ **Î¯Î´Î¹Î¿ key (Î´Î·Î»Î±Î´Î® Ï„Î·Î½ Î¯Î´Î¹Î± Î»Î­Î¾Î·)** Î¸Î± ÏƒÏ„Î±Î»Î¿ÏÎ½ ÏƒÏ„Î¿Î½ Î¯Î´Î¹Î¿ **reducer**, Î¿ Î¿Ï€Î¿Î¯Î¿Ï‚ Î¸Î± Ï„Î± **ÏƒÏ…Î½Î´Ï…Î¬ÏƒÎµÎ¹**.

Î£Ï„Î·Î½ Ï€ÎµÏÎ¯Ï€Ï„Ï‰ÏƒÎ® Î¼Î±Ï‚, ÎµÎ¬Î½ Ï„Î¿ Ï€ÏÏŒÎ³ÏÎ±Î¼Î¼Î± ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÏ„ÎµÎ¯ Ï„Î± ÎµÎ¾Î®Ï‚ Î¶ÎµÏÎ³Î·:

`("text", 1), ("text", 1)`

Î¤ÏŒÏ„Îµ Ï„Î¿ Î±Ï€Î¿Ï„Î­Î»ÎµÏƒÎ¼Î± Ï„Î·Ï‚ ÏƒÏ…Î½Î¬ÏÏ„Î·ÏƒÎ·Ï‚ reduce Î¸Î± ÎµÎ¯Î½Î±Î¹:

`("text", 2)`

Î¤Î­Î»Î¿Ï‚, ÎµÏ†Î±ÏÎ¼ÏŒÎ¶ÎµÏ„Î±Î¹ sortBy Î³Î¹Î± Î½Î± Ï„Î±Î¾Î¹Î½Î¿Î¼Î®ÏƒÎ¿Ï…Î¼Îµ Ï„Î¹Ï‚ Î»Î­Î¾ÎµÎ¹Ï‚ **Î¼Îµ Î²Î¬ÏƒÎ· Ï„Î¿Î½ Î±ÏÎ¹Î¸Î¼ÏŒ ÎµÎ¼Ï†Î±Î½Î¯ÏƒÎµÏ‰Î½ (Ï„Î¹Î¼Î®)** ÎºÎ±Î¹ ÎµÎºÏ„Ï…Ï€ÏÎ½Î¿Ï…Î¼Îµ Ï„Î¿ Ï„ÎµÎ»Î¹ÎºÏŒ Î±Ï€Î¿Ï„Î­Î»ÎµÏƒÎ¼Î±.

**Î‘Ï…Ï„Î® ÎµÎ¯Î½Î±Î¹ Î· Î­Î¾Î¿Î´Î¿Ï‚ Ï„Î¿Ï… Ï€ÏÎ¿Î³ÏÎ¬Î¼Î¼Î±Ï„Î¿Ï‚** wordcount.py:

```python
[('text', 3), ('this', 2), ('is', 2), ('like', 2), ('a', 2), 
 ('file', 2), ('words', 2), (',', 2), ('an', 1), ('of', 1), 
 ('with', 1), ('random', 1), ('example', 1)]
```

Î— Î´Î¹Î±Î´Î¹ÎºÎ±ÏƒÎ¯Î± ÏƒÏ…Î½Î¿Ï€Ï„Î¹ÎºÎ¬ (Ï‡Ï‰ÏÎ¯Ï‚ Î½Î± Ï†Î±Î¯Î½ÎµÏ„Î±Î¹ Ï„Î¿ sortBy):

![Î•Î¹ÎºÏŒÎ½Î± 3](images/img3.png)

## ÎœÎ¿ÏÏ†Î® Î”ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½

Î¤Î¿ Î±ÏÏ‡ÎµÎ¯Î¿ **Employees.csv** Ï€ÎµÏÎ¹Î­Ï‡ÎµÎ¹:

- Ï„Î¿ **ID Ï„Î¿Ï… Ï…Ï€Î±Î»Î»Î®Î»Î¿Ï…**, 
- Ï„Î¿ **ÏŒÎ½Î¿Î¼Î± Ï„Î¿Ï… Ï…Ï€Î±Î»Î»Î®Î»Î¿Ï…**, 
- Ï„Î¿Î½ **Î¼Î¹ÏƒÎ¸ÏŒ** Ï„Î¿Ï… ÎºÎ±Î¹ 
- Ï„Î¿ **ID Ï„Î¿Ï… Ï„Î¼Î®Î¼Î±Ï„Î¿Ï‚** ÏƒÏ„Î¿ Î¿Ï€Î¿Î¯Î¿ ÎµÏÎ³Î¬Î¶ÎµÏ„Î±Î¹.

**Î”Î¿Î¼Î®:**

| ID | ÎŸÎÎŸÎœÎ‘ | ÎœÎ™Î£Î˜ÎŸÎ£ | Î¤ÎœÎ—ÎœÎ‘_ID | 
|----|----|----|----|

Ï€.Ï‡. `1,George R,2000,1`

Î¤Î¿ Î±ÏÏ‡ÎµÎ¯Î¿ **Departments.csv** Ï€ÎµÏÎ¹Î­Ï‡ÎµÎ¹:
Ï„Î¿ **ID Ï„Î¿Ï… Ï„Î¼Î®Î¼Î±Ï„Î¿Ï‚** ÎºÎ±Î¹ Ï„Î¿ **ÏŒÎ½Î¿Î¼Î± Ï„Î¿Ï… Ï„Î¼Î®Î¼Î±Ï„Î¿Ï‚**.

**Î”Î¿Î¼Î®:**

| ID | ÎŸÎÎŸÎœÎ‘ | 
|----|----|


Ï€.Ï‡. `1,Dep A`

**Î•Î¡Î©Î¤Î—ÎœÎ‘ 1:** Î’ÏÎµÏ‚ Ï„Î¿Ï…Ï‚ 5 Ï…Ï€Î±Î»Î»Î®Î»Î¿Ï…Ï‚ Î¼Îµ Ï„Î¿Î½ Ï‡Î±Î¼Î·Î»ÏŒÏ„ÎµÏÎ¿ Î¼Î¹ÏƒÎ¸ÏŒ

**Î•Î¡Î©Î¤Î—ÎœÎ‘ 2**: Î’ÏÎµÏ‚ Ï„Î¿Ï…Ï‚ 3 Ï€Î¹Î¿ ÎºÎ±Î»Î¿Ï€Î»Î·ÏÏ‰Î¼Î­Î½Î¿Ï…Ï‚ Ï…Ï€Î±Î»Î»Î®Î»Î¿Ï…Ï‚ Î±Ï€ÏŒ Ï„Î¿ Ï„Î¼Î®Î¼Î± `Dep A`

**Î•Î¡Î©Î¤Î—ÎœÎ‘ 3**: Î’ÏÎµÏ‚ Ï„Î¿ ÎµÏ„Î®ÏƒÎ¹Î¿ ÎµÎ¹ÏƒÏŒÎ´Î·Î¼Î± ÏŒÎ»Ï‰Î½ Ï„Ï‰Î½ Ï…Ï€Î±Î»Î»Î®Î»Ï‰Î½

**Î•Î¡Î©Î¤Î—ÎœÎ‘ 4**: ÎšÎ¬Î½Îµ ÏƒÏ…Î½Î­Î½Ï‰ÏƒÎ· Ï…Ï€Î±Î»Î»Î®Î»Ï‰Î½ Î¼Îµ Ï„Î¼Î®Î¼Î±Ï„Î± Î¼ÏŒÎ½Î¿ Î¼Îµ Ï„Î·Î½ Ï‡ÏÎ®ÏƒÎ· RDDs

## RDD (Resilient Distributed Datasets)

Î¤Î± **RDDs** Î±Ï€Î¿Ï„ÎµÎ»Î¿ÏÎ½ Ï„Î· **Î¸ÎµÎ¼ÎµÎ»Î¹ÏÎ´Î· Î´Î¿Î¼Î® Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ ÏƒÏ„Î¿ Spark**. Î•Î¯Î½Î±Î¹ **Î±Î¼ÎµÏ„Î¬Î²Î»Î·Ï„ÎµÏ‚**, ÎºÎ±Ï„Î±Î½ÎµÎ¼Î·Î¼Î­Î½ÎµÏ‚ ÏƒÏ…Î»Î»Î¿Î³Î­Ï‚ Î±Î½Ï„Î¹ÎºÎµÎ¹Î¼Î­Î½Ï‰Î½. ÎšÎ¬Î¸Îµ ÏƒÏÎ½Î¿Î»Î¿ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ (RDD) Ï‡Ï‰ÏÎ¯Î¶ÎµÏ„Î±Î¹ ÏƒÎµ **Î»Î¿Î³Î¹ÎºÎ¬ partitions**, Ï„Î± Î¿Ï€Î¿Î¯Î± Î¼Ï€Î¿ÏÎ¿ÏÎ½ Î½Î± ÎµÏ€ÎµÎ¾ÎµÏÎ³Î¬Î¶Î¿Î½Ï„Î±Î¹ ÏƒÎµ **Î´Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÎ¿ÏÏ‚ ÎºÏŒÎ¼Î²Î¿Ï…Ï‚ Ï„Î¿Ï… cluster**. Î¤Î± RDDs Î¼Ï€Î¿ÏÎ¿ÏÎ½ Î½Î± Ï€ÎµÏÎ¹Î­Ï‡Î¿Ï…Î½ **Î¿Ï€Î¿Î¹Î¿Î½Î´Î®Ï€Î¿Ï„Îµ Ï„ÏÏ€Î¿ Î±Î½Ï„Î¹ÎºÎµÎ¹Î¼Î­Î½Ï‰Î½ Python, Java Î® Scala**, Î±ÎºÏŒÎ¼Î± ÎºÎ±Î¹ **ÎºÎ»Î¬ÏƒÎµÎ¹Ï‚ Ï€Î¿Ï… Î¿ÏÎ¯Î¶ÎµÎ¹ Î¿ Ï‡ÏÎ®ÏƒÏ„Î·Ï‚**.

Î‘Ï‚ Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î®ÏƒÎ¿Ï…Î¼Îµ Î­Î½Î± Ï€ÏÏŒÎ³ÏÎ±Î¼Î¼Î± Î³Î¹Î± Ï„Î¿ **Î•ÏÏÏ„Î·Î¼Î± 1** Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÏÎ½Ï„Î±Ï‚ RDDs

Î“Î¹Î± Î½Î± ÎµÎºÏ„ÎµÎ»Î­ÏƒÎµÏ„Îµ Ï„Î¿ Î±ÏÏ‡ÎµÎ¯Î¿ `RddQ1.py`, Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î®ÏƒÏ„Îµ Ï„Î·Î½ ÎµÎ¾Î®Ï‚ ÎµÎ½Ï„Î¿Î»Î®:

```bash
# âš ï¸ Î‘Î½Ï„Î¹ÎºÎ±Ï„Î­ÏƒÏ„Î·ÏƒÎµ Ï„Î¿ "ikons" Î¼Îµ Ï„Î¿ Î´Î¹ÎºÏŒ ÏƒÎ¿Ï… ğŸ‘‡ username
spark-submit hdfs://hdfs-namenode:9000/user/ikons/code/RddQ1.py
```

RddQ1.py:

```python
from pyspark.sql import SparkSession

# âš ï¸ Î‘Î½Ï„Î¹ÎºÎ±Ï„Î­ÏƒÏ„Î·ÏƒÎµ ğŸ‘‡ Ï„Î¿ "ikons" Î¼Îµ Ï„Î¿ Î´Î¹ÎºÏŒ ÏƒÎ¿Ï… username
username = "ikons"
sc = SparkSession \
    .builder \
    .appName("RDD query 1 execution") \
    .getOrCreate() \
    .sparkContext

# Î•Î›Î‘Î§Î™Î£Î¤ÎŸÎ ÎŸÎ™Î—Î£Î— Î•ÎÎŸÎ”Î©Î ÎšÎ‘Î¤Î‘Î“Î¡Î‘Î¦Î—Î£ (LOGGING)
sc.setLogLevel("ERROR")

# Î›Î®ÏˆÎ· Ï„Î¿Ï… job ID ÎºÎ±Î¹ ÎºÎ±Î¸Î¿ÏÎ¹ÏƒÎ¼ÏŒÏ‚ Ï„Î·Ï‚ Î´Î¹Î±Î´ÏÎ¿Î¼Î®Ï‚ ÎµÎ¾ÏŒÎ´Î¿Ï…
job_id = sc.applicationId
output_dir = f"hdfs://hdfs-namenode:9000/user/{username}/RddQ1_{job_id}"

# Î¦ÏŒÏÏ„Ï‰ÏƒÎ· ÎºÎ±Î¹ ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î± Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½
# Î£Ï„Î®Î»ÎµÏ‚ CSV: "id", "name", "salary", "dep_id"
employees = sc.textFile(f"hdfs://hdfs-namenode:9000/user/{username}/examples/employees.csv") \
    .map(lambda x: x.split(","))  # Î”Î¹Î±Ï‡Ï‰ÏÎ¹ÏƒÎ¼ÏŒÏ‚ ÎºÎ¬Î¸Îµ Î³ÏÎ±Î¼Î¼Î®Ï‚ ÏƒÎµ Î»Î¯ÏƒÏ„Î±

# Î‘Î½Ï„Î¹ÏƒÏ„Î¿Î¯Ï‡Î¹ÏƒÎ· ÎºÎ¬Î¸Îµ Ï…Ï€Î±Î»Î»Î®Î»Î¿Ï… ÏƒÏ„Î· Î¼Î¿ÏÏ†Î® (salary, [id, name, dep_id]) ÎºÎ±Î¹ Ï„Î±Î¾Î¹Î½ÏŒÎ¼Î·ÏƒÎ· ÎºÎ±Ï„Î¬ Î¼Î¹ÏƒÎ¸ÏŒ (Î±ÏÎ¾Î¿Ï…ÏƒÎ± ÏƒÎµÎ¹ÏÎ¬)
# Î‘Î½Ï„Î¹ÏƒÏ„Î¿Î¯Ï‡Î¹ÏƒÎ· ÏƒÏ„Î·Î»ÏÎ½:
#   x[0] = id
#   x[1] = name
#   x[2] = salary
#   x[3] = dep_id
sorted_employees = employees.map(lambda x: [int(x[2]), [x[0], x[1], x[3]]]) \
    .sortByKey()

# Î•Î¼Ï†Î¬Î½Î¹ÏƒÎ· Ï„Ï‰Î½ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ (Î³Î¹Î± Î­Î»ÎµÎ³Ï‡Î¿)
for item in sorted_employees.coalesce(1).collect():
    print(item)  # Î Î±ÏÎ¬Î´ÎµÎ¹Î³Î¼Î± ÎµÎ¾ÏŒÎ´Î¿Ï…: [60000, ['123', 'Alice', '5']]

# Î£Ï…Î³Ï‡ÏÎ½ÎµÏ…ÏƒÎ· Î³Î¹Î± Î¼ÎµÎ¯Ï‰ÏƒÎ· Î±ÏÎ¹Î¸Î¼Î¿Ï Î±ÏÏ‡ÎµÎ¯Ï‰Î½ ÎµÎ¾ÏŒÎ´Î¿Ï… ÎºÎ±Î¹ Î±Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· ÏƒÏ„Î¿ HDFS
sorted_employees.coalesce(1).saveAsTextFile(output_dir)
```

Î ÏÏŒÎºÎµÎ¹Ï„Î±Î¹ Î³Î¹Î± Î­Î½Î± Î±Ï€Î»ÏŒ Ï€ÏÏŒÎ³ÏÎ±Î¼Î¼Î± ÏŒÏ€Î¿Ï… Ï€ÏÏÏ„Î± Î´Î¹Î±Î²Î¬Î¶Î¿Ï…Î¼Îµ Ï„Î¿ Î±ÏÏ‡ÎµÎ¯Î¿ **employees.csv** Î±Ï€ÏŒ Ï„Î¿ **HDFS** ÎºÎ±Î¹ ÏƒÏ„Î· ÏƒÏ…Î½Î­Ï‡ÎµÎ¹Î± Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ Ï„Î· ÏƒÏ…Î½Î¬ÏÏ„Î·ÏƒÎ· `map`, Î· Î¿Ï€Î¿Î¯Î± Î´Î·Î¼Î¹Î¿Ï…ÏÎ³ÎµÎ¯ Î¼Î¯Î± **Î»Î¯ÏƒÏ„Î± Î³Î¹Î± ÎºÎ¬Î¸Îµ ÎµÎ³Î³ÏÎ±Ï†Î® Ï…Ï€Î±Î»Î»Î®Î»Î¿Ï…** (Î´Î·Î»Î±Î´Î® Î³Î¹Î± ÎºÎ¬Î¸Îµ Î³ÏÎ±Î¼Î¼Î®).

Î£Îµ Î±Ï…Ï„ÏŒ Ï„Î¿ Ï€Î±ÏÎ¬Î´ÎµÎ¹Î³Î¼Î± Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ `map` Î±Î½Ï„Î¯ Î³Î¹Î± `flatMap` ÎµÏ€ÎµÎ¹Î´Î® Î¼Î±Ï‚ ÎµÎ½Î´Î¹Î±Ï†Î­ÏÎµÎ¹ **ÎºÎ¬Î¸Îµ Î¼ÎµÎ¼Î¿Î½Ï‰Î¼Î­Î½Î· ÎµÎ³Î³ÏÎ±Ï†Î®**, ÎºÎ±Î¹ Î¸Î­Î»Î¿Ï…Î¼Îµ Î½Î± Î¼Ï€Î¿ÏÎ¿ÏÎ¼Îµ Î½Î± Ï„Î·Î½ ÎµÎ½Ï„Î¿Ï€Î¯ÏƒÎ¿Ï…Î¼Îµ ÏƒÏ„Î· ÏƒÏ…Î½Î­Ï‡ÎµÎ¹Î±.

Î— Î­Î¾Î¿Î´Î¿Ï‚ Ï„Î·Ï‚ `map(lambda x: x.split(","))` Î¸Î± ÎµÎ¯Î½Î±Î¹ ÎºÎ¬Ï„Î¹ ÏƒÎ±Î½ Ï„Î¿ Ï€Î±ÏÎ±ÎºÎ¬Ï„Ï‰:

`[ ["id", "name", "salary", "dep_id"],  [...],  [...], ... ]`

Î‘Î½ Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÏƒÎ±Î¼Îµ flatMap ÎµÎ´Ï, Î· Î­Î¾Î¿Î´Î¿Ï‚ Î¸Î± Î®Ï„Î±Î½:

`["id", "name", "salary", "dep_id", "id", "name", "salary", "dep_id", ...]`

(Î”ÎµÎ½ Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½ Î¼ÎµÎ¼Î¿Î½Ï‰Î¼Î­Î½ÎµÏ‚ ÎµÎ³Î³ÏÎ±Ï†Î­Ï‚, Î±Î»Î»Î¬ Î¼Î¯Î± **ÎµÎ½Î¹Î±Î¯Î± Î»Î¯ÏƒÏ„Î±** â€“ ÎµÏ€Î¿Î¼Î­Î½Ï‰Ï‚ **Î´ÎµÎ½ Î¼Ï€Î¿ÏÎ¿ÏÎ¼Îµ Î½Î± ÎºÎ¬Î½Î¿Ï…Î¼Îµ Ï„Î±Î¾Î¹Î½ÏŒÎ¼Î·ÏƒÎ· Î® Î¿Î¼Î±Î´Î¿Ï€Î¿Î¯Î·ÏƒÎ·** Î¼Îµ Î²Î¬ÏƒÎ· ÎºÎ¬Ï€Î¿Î¹Î± Ï„Î¹Î¼Î®.)

Î£Ï„Î· Î´ÎµÏÏ„ÎµÏÎ· ÏƒÏ…Î½Î¬ÏÏ„Î·ÏƒÎ· `map`, ÎºÎ¬Î¸Îµ Ï…Ï€Î¬Î»Î»Î·Î»Î¿Ï‚ Î±Î½Ï„Î¹ÏƒÏ„Î¿Î¹Ï‡Î¯Î¶ÎµÏ„Î±Î¹ ÏƒÎµ Î­Î½Î± **Î¶ÎµÏÎ³Î¿Ï‚ (key, value)**.
Î£Îµ Î±Ï…Ï„Î® Ï„Î·Î½ Ï€ÎµÏÎ¯Ï€Ï„Ï‰ÏƒÎ·, Ï„Î¿ key **ÎµÎ¯Î½Î±Î¹ Î¿ Î¼Î¹ÏƒÎ¸ÏŒÏ‚** Ï„Î¿Ï… Ï…Ï€Î±Î»Î»Î®Î»Î¿Ï… (Ï„Î¿Î½ Î¿Ï€Î¿Î¯Î¿ Î¼ÎµÏ„Î±Ï„ÏÎ­Ï€Î¿Ï…Î¼Îµ ÏƒÎµ int) ÎºÎ±Î¹ Ï„Î¿ value **ÎµÎ¯Î½Î±Î¹ Î¼Î¯Î± Î»Î¯ÏƒÏ„Î± Î¼Îµ Ï„Î± Ï…Ï€ÏŒÎ»Î¿Î¹Ï€Î± ÏƒÏ„Î¿Î¹Ï‡ÎµÎ¯Î±**: `"id", "name", "dep_id"`.

Î£Ï„Î· ÏƒÏ…Î½Î­Ï‡ÎµÎ¹Î±, Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ Ï„Î· ÏƒÏ…Î½Î¬ÏÏ„Î·ÏƒÎ· `sortByKey()` Î³Î¹Î± Î½Î± Ï„Î±Î¾Î¹Î½Î¿Î¼Î®ÏƒÎ¿Ï…Î¼Îµ Ï„Î¿Ï…Ï‚ Ï…Ï€Î±Î»Î»Î®Î»Î¿Ï…Ï‚ **Î²Î¬ÏƒÎµÎ¹ Î¼Î¹ÏƒÎ¸Î¿Ï** ÏƒÎµ **Î±ÏÎ¾Î¿Ï…ÏƒÎ± ÏƒÎµÎ¹ÏÎ¬** (ÎµÎ¯Î½Î±Î¹ Î· Ï€ÏÎ¿ÎµÏ€Î¹Î»Î¿Î³Î®). Î¤Î­Î»Î¿Ï‚, ÎµÎºÏ„Ï…Ï€ÏÎ½Î¿Ï…Î¼Îµ Ï„Î¹Ï‚ **5 Ï€ÏÏÏ„ÎµÏ‚ ÎµÎ³Î³ÏÎ±Ï†Î­Ï‚** Î¼Îµ Ï„Î· ÏƒÏ…Î½Î¬ÏÏ„Î·ÏƒÎ· `take(5)`.

**Î Î±ÏÎ¬Î´ÎµÎ¹Î³Î¼Î± ÎµÎ¾ÏŒÎ´Î¿Ï…:**

```python
[(550, ['6', 'Jerry L', '3']),
 (1000, ['7', 'Marios K', '1']),
 (1000, ['2', 'John K', '2']),
 (1050, ['5', 'Helen K', '2']),
 (1500, ['10', 'Yiannis T', '1'])]
```

Î‘Ï…Ï„Î® ÎµÎ¯Î½Î±Î¹ Î¼Î¹Î± **Ï…Î»Î¿Ï€Î¿Î¯Î·ÏƒÎ· Î¼Îµ RDD** Î³Î¹Î± Ï„Î¿ **ÎµÏÏÏ„Î·Î¼Î± 2**:
Î“Î¹Î± Î½Î± ÎµÎºÏ„ÎµÎ»Î­ÏƒÎµÏ„Îµ Ï„Î¿ Ï€ÏÏŒÎ³ÏÎ±Î¼Î¼Î± `RddQ2.py`, Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î®ÏƒÏ„Îµ Ï„Î·Î½ ÎµÎ¾Î®Ï‚ ÎµÎ½Ï„Î¿Î»Î®:

```bash
# âš ï¸ Î‘Î½Ï„Î¹ÎºÎ±Ï„Î­ÏƒÏ„Î·ÏƒÎµ Ï„Î¿ "ikons" Î¼Îµ Ï„Î¿ Î´Î¹ÎºÏŒ ÏƒÎ¿Ï… ğŸ‘‡ username
spark-submit hdfs://hdfs-namenode:9000/user/ikons/code/RddQ2.py
```

RddQ2.py:

```python
from pyspark.sql import SparkSession

# âš ï¸ Î‘Î½Ï„Î¹ÎºÎ±Ï„Î­ÏƒÏ„Î·ÏƒÎµ ğŸ‘‡ Ï„Î¿ "ikons" Î¼Îµ Ï„Î¿ Î´Î¹ÎºÏŒ ÏƒÎ¿Ï… username
username = "ikons"
sc = SparkSession \
    .builder \
    .appName("RDD query 2 execution") \
    .getOrCreate() \
    .sparkContext

# Î•Î›Î‘Î§Î™Î£Î¤ÎŸÎ ÎŸÎ™Î—Î£Î— Î•ÎÎŸÎ”Î©Î ÎšÎ‘Î¤Î‘Î“Î¡Î‘Î¦Î—Î£ (LOGGING)
sc.setLogLevel("ERROR")

# Î›Î®ÏˆÎ· Ï„Î¿Ï… job ID ÎºÎ±Î¹ ÎºÎ±Î¸Î¿ÏÎ¹ÏƒÎ¼ÏŒÏ‚ Ï„Î·Ï‚ Î´Î¹Î±Î´ÏÎ¿Î¼Î®Ï‚ ÎµÎ¾ÏŒÎ´Î¿Ï…
job_id = sc.applicationId
output_dir = f"hdfs://hdfs-namenode:9000/user/{username}/RddQ2_{job_id}"

# =======================
# Î Î›Î—Î¡ÎŸÎ¦ÎŸÎ¡Î™Î•Î£ Î£Î§Î—ÎœÎ‘Î¤ÎŸÎ£:
# employees:   "emp_id", "emp_name", "salary", "dep_id"
# departments: "id", "dpt_name"
#
# Î‘Î½Ï„Î¹ÏƒÏ„Î¿Î¯Ï‡Î¹ÏƒÎ· Î¸Î­ÏƒÎµÏ‰Î½ Î³Î¹Î± employees:
#   x[0] = emp_id
#   x[1] = emp_name
#   x[2] = salary
#   x[3] = dep_id
#
# Î‘Î½Ï„Î¹ÏƒÏ„Î¿Î¯Ï‡Î¹ÏƒÎ· Î¸Î­ÏƒÎµÏ‰Î½ Î³Î¹Î± departments:
#   x[0] = id
#   x[1] = dpt_name
# =======================

# Î¦ÏŒÏÏ„Ï‰ÏƒÎ· ÎºÎ±Î¹ Î±Î½Î¬Î»Ï…ÏƒÎ· Ï„Ï‰Î½ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ Ï…Ï€Î±Î»Î»Î®Î»Ï‰Î½
employees = sc.textFile("hdfs://hdfs-namenode:9000/user/ikons/examples/employees.csv") \
    .map(lambda x: x.split(","))  # â†’ [emp_id, emp_name, salary, dep_id]

# Î¦ÏŒÏÏ„Ï‰ÏƒÎ· ÎºÎ±Î¹ Î±Î½Î¬Î»Ï…ÏƒÎ· Ï„Ï‰Î½ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ Ï„Î¼Î·Î¼Î¬Ï„Ï‰Î½
departments = sc.textFile("hdfs://hdfs-namenode:9000/user/ikons/examples/departments.csv") \
    .map(lambda x: x.split(","))  # â†’ [id, dpt_name]

# Î¦Î¹Î»Ï„ÏÎ¬ÏÎ¹ÏƒÎ¼Î± Î¼ÏŒÎ½Î¿ Ï„Ï‰Î½ Ï„Î¼Î·Î¼Î¬Ï„Ï‰Î½ Î¼Îµ dpt_name == "Dep A"
depA = departments.filter(lambda x: x[1] == "Dep A")

# ÎœÎ¿ÏÏ†Î¿Ï€Î¿Î¯Î·ÏƒÎ· Ï…Ï€Î±Î»Î»Î®Î»Ï‰Î½ ÏƒÎµ (dep_id, [emp_id, emp_name, salary])
# Î§ÏÎ®ÏƒÎ· Ï„Î¿Ï… x[3] = dep_id Ï‰Ï‚ ÎºÎ»ÎµÎ¹Î´Î¯
employees_formatted = employees.map(lambda x: [x[3], [x[0], x[1], x[2]]])

# ÎœÎ¿ÏÏ†Î¿Ï€Î¿Î¯Î·ÏƒÎ· Ï„Î¼Î·Î¼Î¬Ï„Ï‰Î½ ÏƒÎµ (id, [dpt_name])
# Î§ÏÎ®ÏƒÎ· Ï„Î¿Ï… x[0] = id Ï‰Ï‚ ÎºÎ»ÎµÎ¹Î´Î¯
depA_formatted = depA.map(lambda x: [x[0], [x[1]]])

# Î£Ï…Î½Î­Î½Ï‰ÏƒÎ· Ï…Ï€Î±Î»Î»Î®Î»Ï‰Î½ Î¼Îµ Ï„Î¿ Ï„Î¼Î®Î¼Î± "Dep A" Î²Î¬ÏƒÎµÎ¹ dep_id
# Î‘Ï€Î¿Ï„Î­Î»ÎµÏƒÎ¼Î±: (dep_id, ([emp_id, emp_name, salary], [dpt_name]))
joined_data = employees_formatted.join(depA_formatted)

# Î•Î¾Î±Î³Ï‰Î³Î® Î¼ÏŒÎ½Î¿ Ï„Ï‰Î½ ÏƒÏ„Î¿Î¹Ï‡ÎµÎ¯Ï‰Î½ Ï…Ï€Î±Î»Î»Î®Î»Ï‰Î½ (Ï‡Ï‰ÏÎ¯Ï‚ Ï„Î± ÏƒÏ„Î¿Î¹Ï‡ÎµÎ¯Î± Ï„Î¿Ï… Ï„Î¼Î®Î¼Î±Ï„Î¿Ï‚)
# Î‘Ï€Î¿Ï„Î­Î»ÎµÏƒÎ¼Î±: [emp_id, emp_name, salary]
get_employees = joined_data.map(lambda x: x[1][0])

# Î¤Î±Î¾Î¹Î½ÏŒÎ¼Î·ÏƒÎ· Ï…Ï€Î±Î»Î»Î®Î»Ï‰Î½ ÎºÎ±Ï„Î¬ Ï†Î¸Î¯Î½Î¿Ï…ÏƒÎ± ÏƒÎµÎ¹ÏÎ¬ Î¼Î¹ÏƒÎ¸Î¿Ï
# Î•Î¯ÏƒÎ¿Î´Î¿Ï‚: [emp_id, emp_name, salary] â€” x[2] = salary
# ÎˆÎ¾Î¿Î´Î¿Ï‚: (salary, [emp_id, emp_name])
sorted_employees = get_employees.map(lambda x: [int(x[2]), [x[0], x[1]]]) \
    .sortByKey(ascending=False)

# Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± RDD Î¼Îµ Î´Î¹Î±Ï‡Ï‰ÏÎ¹ÏƒÏ„Î¹ÎºÎ® Î³ÏÎ±Î¼Î¼Î® Î³Î¹Î± Ï„Î·Î½ Ï„ÎµÎ»Î¹ÎºÎ® Î­Î¾Î¿Î´Î¿
delimiter = ["=========="]
delimiter_rdd = sc.parallelize(delimiter)  # RDD Î¼Î¯Î±Ï‚ Î³ÏÎ±Î¼Î¼Î®Ï‚

# Î£Ï…Î½Î­Î½Ï‰ÏƒÎ· ÏŒÎ»Ï‰Î½ Ï„Ï‰Î½ RDD Î¼Îµ Î´Î¹Î±Ï‡Ï‰ÏÎ¹ÏƒÏ„Î¹ÎºÎ¬ ÎµÎ½Î´Î¹Î¬Î¼ÎµÏƒÎ±
final_rdd = employees_formatted.union(delimiter_rdd) \
    .union(departments) \
    .union(delimiter_rdd) \
    .union(joined_data) \
    .union(delimiter_rdd) \
    .union(sorted_employees)

# Î•Î¼Ï†Î¬Î½Î¹ÏƒÎ· Ï„Î·Ï‚ Ï„ÎµÎ»Î¹ÎºÎ®Ï‚ ÎµÎ¾ÏŒÎ´Î¿Ï… (Î³Î¹Î± Î´Î¿ÎºÎ¹Î¼Î®/debugging)
for item in final_rdd.coalesce(1).collect():
    print(item)

# Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· Ï„Î·Ï‚ Ï„ÎµÎ»Î¹ÎºÎ®Ï‚ ÎµÎ¾ÏŒÎ´Î¿Ï… ÏƒÏ„Î¿ HDFS
final_rdd.coalesce(1).saveAsTextFile(output_dir)
```

Î‘ÏÏ‡Î¹ÎºÎ¬, Î´Î¹Î±Î²Î¬Î¶Î¿Ï…Î¼Îµ Ï„Î± Î´ÏÎ¿ Î±ÏÏ‡ÎµÎ¯Î± **CSV** Î±Ï€ÏŒ Ï„Î¿ HDFS ÎºÎ±Î¹ Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î¿ÏÎ¼Îµ Î¼Î¯Î± Î»Î¯ÏƒÏ„Î± Î¼Îµ Ï„Î¹Ï‚ Ï„Î¹Î¼Î­Ï‚ ÎºÎ¬Î¸Îµ ÎµÎ³Î³ÏÎ±Ï†Î®Ï‚. 

Î£Ï„Î· ÏƒÏ…Î½Î­Ï‡ÎµÎ¹Î±, Ï€ÏÎ­Ï€ÎµÎ¹ Î½Î± Î²ÏÎ¿ÏÎ¼Îµ Ï€Î¿Î¹Î¿ ÎµÎ¯Î½Î±Î¹ Ï„Î¿ **ID Ï„Î¿Ï… "Dep A"**. Î‘Ï…Ï„ÏŒ Ï„Î¿ ÎµÏ€Î¹Ï„Ï…Î³Ï‡Î¬Î½Î¿Ï…Î¼Îµ Î¼Îµ Ï‡ÏÎ®ÏƒÎ· Ï„Î·Ï‚ ÏƒÏ…Î½Î¬ÏÏ„Î·ÏƒÎ·Ï‚ `map`, Î· Î¿Ï€Î¿Î¯Î± ÎµÎ»Î­Î³Ï‡ÎµÎ¹ Î±Î½ Ï„Î¿ ÏŒÎ½Î¿Î¼Î± Ï„Î¿Ï… Ï„Î¼Î®Î¼Î±Ï„Î¿Ï‚ ÎµÎ¯Î½Î±Î¹ "Dep A". Î‘Î½ ÎµÎ¯Î½Î±Î¹ **Î±Î»Î·Î¸Î­Ï‚**, ÎµÏ€Î¹ÏƒÏ„ÏÎ­Ï†Î¿Ï…Î¼Îµ Ï„Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î± Ï„Î·Ï‚ ÎµÎ³Î³ÏÎ±Ï†Î®Ï‚ â€“ Î±Î½ ÎµÎ¯Î½Î±Î¹ **ÏˆÎµÏ…Î´Î­Ï‚**, ÎµÏ€Î¹ÏƒÏ„ÏÎ­Ï†Î¿Ï…Î¼Îµ `None`. ÎˆÏ€ÎµÎ¹Ï„Î±, Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ `filter` Î³Î¹Î± Î½Î± Î±Ï€Î¿ÏÏÎ¯ÏˆÎ¿Ï…Î¼Îµ Ï„Î¹Ï‚ ÎµÎ³Î³ÏÎ±Ï†Î­Ï‚ Ï€Î¿Ï… Ï€ÎµÏÎ¹Î­Ï‡Î¿Ï…Î½ `None`. ÎŸÏ…ÏƒÎ¹Î±ÏƒÏ„Î¹ÎºÎ¬ **Î±Î½Ï„Î¹ÎºÎ±Î¸Î¹ÏƒÏ„Î¿ÏÎ¼Îµ ÏŒÎ»Î± Ï„Î± Ï„Î¼Î®Î¼Î±Ï„Î± Ï€Î¿Ï… Î´ÎµÎ½ ÎµÎ¯Î½Î±Î¹ Ï„Î¿ "Dep A" Î¼Îµ Ï„Î¹Î¼Î­Ï‚ None ÎºÎ±Î¹ Ï„Î¹Ï‚ Ï†Î¹Î»Ï„ÏÎ¬ÏÎ¿Ï…Î¼Îµ**.

Î£Ï„Î· ÏƒÏ…Î½Î­Ï‡ÎµÎ¹Î±, Ï‡ÏÎµÎ¹Î¬Î¶ÎµÏ„Î±Î¹ Î½Î± ÎºÎ¬Î½Î¿Ï…Î¼Îµ **Î­Î½Ï‰ÏƒÎ· (join)** Ï„Ï‰Î½ Î´ÏÎ¿ Ï€Î¹Î½Î¬ÎºÏ‰Î½ Î²Î¬ÏƒÎµÎ¹ Ï„Ï‰Î½ Ï€ÎµÎ´Î¯Ï‰Î½ `dep_id` ÎºÎ±Î¹ `id`. Î— Î­Î½Ï‰ÏƒÎ· Î³Î¯Î½ÎµÏ„Î±Î¹ Î¼Îµ Ï‡ÏÎ®ÏƒÎ· Ï„Î·Ï‚ `join` Ï…Î»Î¿Ï€Î¿Î¯Î·ÏƒÎ·Ï‚ Ï„Î¿Ï… **RDD API**.

Î“Î¹Î± Î½Î± Î³Î¯Î½ÎµÎ¹ Î±Ï…Ï„ÏŒ, Ï€ÏÏÏ„Î± Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î¿ÏÎ¼Îµ Î­Î½Î± **Î¶ÎµÏÎ³Î¿Ï‚ (key, value)** Î³Î¹Î± Ï„Î¿Ï…Ï‚ Ï…Ï€Î±Î»Î»Î®Î»Î¿Ï…Ï‚:

- **key = dep_id**
- **value = Î»Î¯ÏƒÏ„Î± Î¼Îµ Ï„Î¹Ï‚ Ï…Ï€ÏŒÎ»Î¿Î¹Ï€ÎµÏ‚ Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯ÎµÏ‚ Ï…Ï€Î±Î»Î»Î®Î»Î¿Ï…**

Î¤Î¿ Î¯Î´Î¹Î¿ ÎºÎ¬Î½Î¿Ï…Î¼Îµ ÎºÎ±Î¹ Î³Î¹Î± Ï„Î± Ï„Î¼Î®Î¼Î±Ï„Î± (departments):

- **key = id**
- **value = ÏŒÎ½Î¿Î¼Î± Ï„Î¼Î®Î¼Î±Ï„Î¿Ï‚**

ÎˆÏ€ÎµÎ¹Ï„Î± ÎºÎ¬Î½Î¿Ï…Î¼Îµ `join` (ÏƒÏ…Î½Î­Î½Ï‰ÏƒÎ·) Ï„Ï‰Î½ Î´ÏÎ¿ RDDs. Î¤Î¿ Î½Î­Î¿ RDD joined_data Ï€ÎµÏÎ¹Î­Ï‡ÎµÎ¹ **Î¼ÏŒÎ½Î¿ Ï„Î¿Ï…Ï‚ Ï…Ï€Î±Î»Î»Î®Î»Î¿Ï…Ï‚ Ï€Î¿Ï… Î±Î½Î®ÎºÎ¿Ï…Î½ ÏƒÏ„Î¿ "Dep A"**:

```python
[
 ('1', (['7', 'Marios K', '1000'], ['Dep A'])),
 ('1', (['10', 'Yiannis T', '1500'], ['Dep A'])),
 ('1', (['1', 'George R', '2000'], ['Dep A'])),
 ('1', (['3', 'Mary T', '2100'], ['Dep A'])),
 ('1', (['4', 'George T', '2100'], ['Dep A']))
]
```

Î¤ÏÏÎ± Î¼Ï€Î¿ÏÎ¿ÏÎ¼Îµ Î½Î± **Î±Ï†Î±Î¹ÏÎ­ÏƒÎ¿Ï…Î¼Îµ Ï„Î¿ `dep_id` ÎºÎ±Î¹ Ï„Î¿ ÏŒÎ½Î¿Î¼Î± Ï„Î¼Î®Î¼Î±Ï„Î¿Ï‚**. Î¤Î¿ ÎºÎ¬Î½Î¿Ï…Î¼Îµ Î±Ï…Ï„ÏŒ Î¼Î­ÏƒÏ‰ Ï„Î¿Ï… `get_employees` RDD:

```python
[
 ['1', 'George R', '2000'],
 ['3', 'Mary T', '2100'],
 ['4', 'George T', '2100'],
 ['7', 'Marios K', '1000'],
 ['10', 'Yiannis T', '1500']
]
```

Î£Ï„Î¿ Ï„Î­Î»Î¿Ï‚, Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î¿ÏÎ¼Îµ Î¶ÎµÏÎ³Î· **(key, value)** ÏŒÏ€Î¿Ï…:

- key = Î¼Î¹ÏƒÎ¸ÏŒÏ‚
- value = Ï„Î± Ï…Ï€ÏŒÎ»Î¿Î¹Ï€Î± ÏƒÏ„Î¿Î¹Ï‡ÎµÎ¯Î± Ï„Î¿Ï… Ï…Ï€Î±Î»Î»Î®Î»Î¿Ï…

Î¤Î± Ï„Î±Î¾Î¹Î½Î¿Î¼Î¿ÏÎ¼Îµ Î¼Îµ Î²Î¬ÏƒÎ· Ï„Î¿ key ÏƒÎµ **Ï†Î¸Î¯Î½Î¿Ï…ÏƒÎ± ÏƒÎµÎ¹ÏÎ¬** Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÏÎ½Ï„Î±Ï‚ `sortByKey()` ÎºÎ±Î¹ ÎµÎºÏ„Ï…Ï€ÏÎ½Î¿Ï…Î¼Îµ ÏŒÎ»ÎµÏ‚ Ï„Î¹Ï‚ ÎµÎ³Î³ÏÎ±Ï†Î­Ï‚.

```python
(2100, ['3', 'Mary T'])
(2100, ['4', 'George T'])
(2000, ['1', 'George R'])
(1500, ['10', 'Yiannis T'])
(1000, ['7', 'Marios K'])
```


Î‘Ï…Ï„Î® ÎµÎ¯Î½Î±Î¹ Î¼Î¹Î± Ï…Î»Î¿Ï€Î¿Î¯Î·ÏƒÎ· Î¼Îµ RDD Î³Î¹Î± Ï„Î¿ ÎµÏÏÏ„Î·Î¼Î± 3:
Î“Î¹Î± Î½Î± ÎµÎºÏ„ÎµÎ»Î­ÏƒÎµÏ„Îµ Ï„Î¿ Ï€ÏÏŒÎ³ÏÎ±Î¼Î¼Î± `RddQ3.py`, Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î®ÏƒÏ„Îµ Ï„Î·Î½ ÎµÎ¾Î®Ï‚ ÎµÎ½Ï„Î¿Î»Î®:

```bash
# âš ï¸ Î‘Î½Ï„Î¹ÎºÎ±Ï„Î­ÏƒÏ„Î·ÏƒÎµ Ï„Î¿ "ikons" Î¼Îµ Ï„Î¿ Î´Î¹ÎºÏŒ ÏƒÎ¿Ï… ğŸ‘‡ username
spark-submit hdfs://hdfs-namenode:9000/user/ikons/code/RddQ3.py
```

RddQ3.py:

```python
from pyspark.sql import SparkSession

# âš ï¸ Î‘Î½Ï„Î¹ÎºÎ±Ï„Î­ÏƒÏ„Î·ÏƒÎµ ğŸ‘‡ Ï„Î¿ "ikons" Î¼Îµ Ï„Î¿ Î´Î¹ÎºÏŒ ÏƒÎ¿Ï… username
username = "ikons"
sc = SparkSession \
    .builder \
    .appName("RDD query 3 execution") \
    .getOrCreate() \
    .sparkContext

# Î•Î›Î‘Î§Î™Î£Î¤ÎŸÎ ÎŸÎ™Î—Î£Î— Î•ÎÎŸÎ”Î©Î ÎšÎ‘Î¤Î‘Î“Î¡Î‘Î¦Î—Î£ (LOGGING)
sc.setLogLevel("ERROR")

# Î›Î®ÏˆÎ· Ï„Î¿Ï… job ID ÎºÎ±Î¹ ÎºÎ±Î¸Î¿ÏÎ¹ÏƒÎ¼ÏŒÏ‚ Ï„Î·Ï‚ Î´Î¹Î±Î´ÏÎ¿Î¼Î®Ï‚ ÎµÎ¾ÏŒÎ´Î¿Ï…
job_id = sc.applicationId
output_dir = f"hdfs://hdfs-namenode:9000/user/{username}/RddQ3_{job_id}"

# =======================
# Î Î›Î—Î¡ÎŸÎ¦ÎŸÎ¡Î™Î•Î£ Î£Î§Î—ÎœÎ‘Î¤ÎŸÎ£:
# employees:   "emp_id", "emp_name", "salary", "dep_id"
# departments: "id", "dpt_name"
#
# Î‘Î½Ï„Î¹ÏƒÏ„Î¿Î¯Ï‡Î¹ÏƒÎ· Î¸Î­ÏƒÎµÏ‰Î½ Î³Î¹Î± employees:
#   x[0] = emp_id
#   x[1] = emp_name
#   x[2] = salary
#   x[3] = dep_id
#
# Î‘Î½Ï„Î¹ÏƒÏ„Î¿Î¯Ï‡Î¹ÏƒÎ· Î¸Î­ÏƒÎµÏ‰Î½ Î³Î¹Î± departments:
#   x[0] = id
#   x[1] = dpt_name
# =======================

# Î¦ÏŒÏÏ„Ï‰ÏƒÎ· ÎºÎ±Î¹ Î±Î½Î¬Î»Ï…ÏƒÎ· Ï„Ï‰Î½ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ Ï…Ï€Î±Î»Î»Î®Î»Ï‰Î½
employees = sc.textFile("hdfs://hdfs-namenode:9000/user/ikons/examples/employees.csv") \
    .map(lambda x: x.split(","))  # â†’ [emp_id, emp_name, salary, dep_id]
# ÎšÎ±Ï„ÎµÏ…Î¸ÎµÎ¯Î±Î½ Ï…Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ Ï„Ï‰Î½ ÎµÏ„Î®ÏƒÎ¹Ï‰Î½ ÎµÎ¹ÏƒÎ¿Î´Î·Î¼Î¬Ï„Ï‰Î½ Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÏÎ½Ï„Î±Ï‚ lambda function:
employees_yearly_income = employees \
    .map (lambda x: [x[1]), 14*(int(x[2]))]) # â†’ [emp_name, 14*salary]
# Î•Î¼Ï†Î¬Î½Î¹ÏƒÎ· Ï„Î·Ï‚ Ï„ÎµÎ»Î¹ÎºÎ®Ï‚ ÎµÎ¾ÏŒÎ´Î¿Ï… (Î³Î¹Î± Î´Î¿ÎºÎ¹Î¼Î®/debugging)
for item in employees_yearly_income.coalesce(1).collect():
    print(item)
# Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· Ï„Î·Ï‚ Ï„ÎµÎ»Î¹ÎºÎ®Ï‚ ÎµÎ¾ÏŒÎ´Î¿Ï… ÏƒÏ„Î¿ HDFS
employees_yearly_income.coalesce(1).saveAsTextFile(output_dir)

```



**RddQ4 Î Î±ÏÎ¬Î´ÎµÎ¹Î³Î¼Î± Hands-On: ÎˆÎ½Ï‰ÏƒÎ· Î´ÏÎ¿ ÏƒÏ…Î½ÏŒÎ»Ï‰Î½ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÏÎ½Ï„Î±Ï‚ RDDs (Î¼ÏŒÎ½Î¿ Î¼Îµ Map/Reduce jobs):**

**Dataset A**

```python
(1, George K, 1)
(2, John T, 2)
(3, Mary M, 1)
(4, Jerry S, 3)
```

**Dataset B**

```python
(1, Dep A)
(2, Dep B)
(3, Dep C)
```

Î¤Î¿ Dataset A Ï€ÎµÏÎ¹Î­Ï‡ÎµÎ¹:

- ID Ï…Ï€Î±Î»Î»Î®Î»Î¿Ï…
- ÎŒÎ½Î¿Î¼Î± Ï…Ï€Î±Î»Î»Î®Î»Î¿Ï…
- ID Ï„Î¼Î®Î¼Î±Ï„Î¿Ï‚

Î¤Î¿ Dataset B Ï€ÎµÏÎ¹Î­Ï‡ÎµÎ¹:

- ID Ï„Î¼Î®Î¼Î±Ï„Î¿Ï‚
- ÎŒÎ½Î¿Î¼Î± Ï„Î¼Î®Î¼Î±Ï„Î¿Ï‚

Î˜Î­Î»Î¿Ï…Î¼Îµ Î½Î± ÎµÎ½ÏÏƒÎ¿Ï…Î¼Îµ Ï„Î± Î´ÏÎ¿ datasets **Î²Î¬ÏƒÎµÎ¹ Ï„Î¿Ï… ID Ï„Î¼Î®Î¼Î±Ï„Î¿Ï‚**.

ÎšÎ±Î¸Î¿ÏÎ¯Î¶Î¿Ï…Î¼Îµ Ï„Î¿ ÎºÎ»ÎµÎ¹Î´Î¯ Î­Î½Ï‰ÏƒÎ·Ï‚ (join key):

Î˜Î± ÎµÎ½ÏÏƒÎ¿Ï…Î¼Îµ Ï„Î± ÏƒÏÎ½Î¿Î»Î± Î²Î¬ÏƒÎµÎ¹ Ï„Î¿Ï… `department_id`. Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ Ï„Î· ÏƒÏ…Î½Î¬ÏÏ„Î·ÏƒÎ· `keyBy()` Î³Î¹Î± Î½Î± Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î®ÏƒÎ¿Ï…Î¼Îµ Î¶ÎµÏÎ³Î· key-value:

**Dataset A**

```
(1, (1, George K, 1))
(2, (2, John T, 2))
(1, (3, Mary M, 1))
(3, (4, Jerry S, 3))
```

**Dataset B**

```
(1, (1, Dep A))
(2, (2, Dep B))
(3, (3, Dep C))
```

Î ÏÎ¿ÏƒÎ¸Î­Ï„Î¿Ï…Î¼Îµ â€œÎµÏ„Î¹ÎºÎ­Ï„Î±â€ Î³Î¹Î± Î½Î± Î¾ÎµÏ‡Ï‰ÏÎ¯Î¶Î¿Ï…Î¼Îµ Ï„Î·Î½ Ï€ÏÎ¿Î­Î»ÎµÏ…ÏƒÎ· ÎºÎ¬Î¸Îµ ÎµÎ³Î³ÏÎ±Ï†Î®Ï‚:

- Î”Î¯Î½Î¿Ï…Î¼Îµ Ï„Î¹Î¼Î® 1 Î³Î¹Î± ÎµÎ³Î³ÏÎ±Ï†Î­Ï‚ Ï„Î¿Ï… Dataset A (Î±ÏÎ¹ÏƒÏ„ÎµÏÏŒ)
- Î¤Î¹Î¼Î® 2 Î³Î¹Î± Ï„Î¿ Dataset B (Î´ÎµÎ¾Î¯)

ÎÎ­ÎµÏ‚ Î¼Î¿ÏÏ†Î­Ï‚

```
(1, (1, (1, George K, 1)))   # Î±Ï€ÏŒ A
(2, (1, (2, John T, 2)))
(1, (1, (3, Mary M, 1)))
(3, (1, (4, Jerry S, 3)))

(1, (2, (1, Dep A)))         # Î±Ï€ÏŒ B
(2, (2, (2, Dep B)))
(3, (2, (3, Dep C)))
```


ÎšÎ¬Î½Î¿Ï…Î¼Îµ Î­Î½Ï‰ÏƒÎ· Î¼Îµ Ï„Î· ÏƒÏ…Î½Î¬ÏÏ„Î·ÏƒÎ· `union`

```python
unioned_data = left.union(right)
```

Î‘Ï€Î¿Ï„Î­Î»ÎµÏƒÎ¼Î±:

```python
(1, (1, (1, George K, 1)))
(2, (1, (2, John T, 2)))
(1, (1, (3, Mary M, 1)))
(3, (1, (4, Jerry S, 3)))
(1, (2, (1, Dep A)))
(2, (2, (2, Dep B)))
(3, (2, (3, Dep C)))
```

ÎŸÎ¼Î±Î´Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ Ï„Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î± Î¼Îµ [`groupByKey()`](  https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.groupByKey.html), ÏÏƒÏ„Îµ ÎµÎ³Î³ÏÎ±Ï†Î­Ï‚ Î¼Îµ Î¯Î´Î¹Î¿ `key` Î½Î± Î²ÏÎµÎ¸Î¿ÏÎ½ ÏƒÏ„Î¿Î½ Î¯Î´Î¹Î¿ reducer:

```python
(1, [(1, (1, George K, 1)), (1, (3, Mary M, 1)), (2, (1, Dep A))])
(2, [(1, (2, John T, 2)), (2, (2, Dep B))])
(3, [(1, (4, Jerry S, 3)), (2, (3, Dep C))])
```

ÎŸÏÎ¯Î¶Î¿Ï…Î¼Îµ ÏƒÏ…Î½Î¬ÏÏ„Î·ÏƒÎ· `arrange()` Î³Î¹Î± Î½Î± Î´Î¹Î±Ï‡Ï‰ÏÎ¯ÏƒÎ¿Ï…Î¼Îµ ÎµÎ³Î³ÏÎ±Ï†Î­Ï‚ Î±Î½Î¬ dataset ÎºÎ±Î¹ Î½Î± ÎµÎ½Î¿Ï€Î¿Î¹Î®ÏƒÎ¿Ï…Î¼Îµ:

```python
def arrange(seq):
    left_origin = []
    right_origin = []
    for (n, v) in seq:
        if n == 1:
            left_origin.append(v)
        elif n == 2:
            right_origin.append(v)
    return [(v, w) for v in left_origin for w in right_origin]
```

Î•Ï†Î±ÏÎ¼ÏŒÎ¶Î¿Ï…Î¼Îµ `flatMapValues(lambda x: arrange(x))`

```python
[
 (3, (4, Jerry S, 3), (3, Dep C)),
 (1, (1, George K, 1), (1, Dep A)),
 (1, (3, Mary M, 1), (1, Dep A)),
 (2, (2, John T, 2), (2, Dep B))
]
```


ÎŸ ÎºÏÎ´Î¹ÎºÎ±Ï‚ ÏƒÎµ python Ï„Î¿Ï… Ï€Î±ÏÎ±Î´ÎµÎ¯Î³Î¼Î±Ï„Î¿Ï‚ ÎµÎ¯Î½Î±Î¹ Î¿ Î±ÎºÏŒÎ»Î¿Ï…Î¸Î¿Ï‚

RddQ4.py

```python
from pyspark.sql import SparkSession

# Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± SparkContext

sc = SparkSession \
    .builder \
    .appName("Join Datasets with RDD") \
    .getOrCreate() \
    .sparkContext

# Î•Î›Î‘Î§Î™Î£Î¤ÎŸÎ ÎŸÎ™Î—Î£Î— Î•ÎÎŸÎ”Î©Î ÎšÎ‘Î¤Î‘Î“Î¡Î‘Î¦Î—Î£ (LOGGING)
sc.setLogLevel("ERROR")


# --------------------------
# Î‘ÏÏ‡Î¹ÎºÎ¬ Î´ÎµÎ´Î¿Î¼Î­Î½Î±
# --------------------------

# Dataset A: (employee_id, employee_name, department_id)
data_a = [
    (1, "George K", 1),
    (2, "John T", 2),
    (3, "Mary M", 1),
    (4, "Jerry S", 3)
]

# Dataset B: (department_id, department_name)
data_b = [
    (1, "Dep A"),
    (2, "Dep B"),
    (3, "Dep C")
]

# --------------------------
# Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± RDDs
# --------------------------

rdd_a = sc.parallelize(data_a)
rdd_b = sc.parallelize(data_b)

# --------------------------
# Î ÏÎ¿ÎµÏ„Î¿Î¹Î¼Î±ÏƒÎ¯Î± Î³Î¹Î± Î­Î½Ï‰ÏƒÎ·
# --------------------------

# Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¿ÏÎ¼Îµ key-value Î¶ÎµÏÎ³Î· Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÏÎ½Ï„Î±Ï‚ Ï„Î¿ department_id Ï‰Ï‚ ÎºÎ»ÎµÎ¹Î´Î¯
# ÎºÎ±Î¹ Ï€ÏÎ¿ÏƒÎ¸Î­Ï„Î¿Ï…Î¼Îµ "ÎµÏ„Î¹ÎºÎ­Ï„Î±" Î³Î¹Î± Ï„Î¿ dataset (1 Î³Î¹Î± A, 2 Î³Î¹Î± B)

# Î‘Ï€ÏŒ Ï„Î¿ Dataset A:
# (department_id, (1, (employee_id, employee_name, department_id)))
left = rdd_a.map(lambda x: (x[2], (1, x)))

# Î‘Ï€ÏŒ Ï„Î¿ Dataset B:
# (department_id, (2, (department_id, department_name)))
right = rdd_b.map(lambda x: (x[0], (2, x)))

# --------------------------
# ÎˆÎ½Ï‰ÏƒÎ· Ï„Ï‰Î½ Î´ÏÎ¿ RDDs
# --------------------------

# ÎšÎ¬Î½Î¿Ï…Î¼Îµ union Î³Î¹Î± Î½Î± Î²ÏÎµÎ¸Î¿ÏÎ½ ÏŒÎ»ÎµÏ‚ Î¿Î¹ ÎµÎ³Î³ÏÎ±Ï†Î­Ï‚ ÏƒÏ„Î¿ Î¯Î´Î¹Î¿ RDD
unioned_data = left.union(right)

# --------------------------
# ÎŸÎ¼Î±Î´Î¿Ï€Î¿Î¯Î·ÏƒÎ· Î²Î¬ÏƒÎµÎ¹ Ï„Î¿Ï… department_id
# --------------------------

# ÎŸÎ¹ ÎµÎ³Î³ÏÎ±Ï†Î­Ï‚ Î¼Îµ Ï„Î¿ Î¯Î´Î¹Î¿ department_id Î¸Î± Î¼Î±Î¶ÎµÏ…Ï„Î¿ÏÎ½ Î¼Î±Î¶Î¯
grouped = unioned_data.groupByKey()

# --------------------------
# Î£Ï…Î½Î¬ÏÏ„Î·ÏƒÎ· Î³Î¹Î± ÎµÎ½Î¿Ï€Î¿Î¯Î·ÏƒÎ· Ï„Ï‰Î½ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½
# --------------------------

def arrange(records):
    left_origin = []   # Î•Î³Î³ÏÎ±Ï†Î­Ï‚ Î±Ï€ÏŒ Ï„Î¿ Dataset A (Ï…Ï€Î¬Î»Î»Î·Î»Î¿Î¹)
    right_origin = []  # Î•Î³Î³ÏÎ±Ï†Î­Ï‚ Î±Ï€ÏŒ Ï„Î¿ Dataset B (Ï„Î¼Î®Î¼Î±Ï„Î±)
    
    for (source_id, value) in records:
        if source_id == 1:
            left_origin.append(value)
        elif source_id == 2:
            right_origin.append(value)

    # Î•Ï€Î¹ÏƒÏ„ÏÎ­Ï†Î¿Ï…Î¼Îµ ÎºÎ¬Î¸Îµ ÏƒÏ…Î½Î´Ï…Î±ÏƒÎ¼ÏŒ Ï…Ï€Î±Î»Î»Î®Î»Î¿Ï… Î¼Îµ ÏŒÎ½Î¿Î¼Î± Ï„Î¼Î®Î¼Î±Ï„Î¿Ï‚
    return [(employee, dept) for employee in left_origin for dept in right_origin]

# --------------------------
# Î¤ÎµÎ»Î¹ÎºÏŒ Î±Ï€Î¿Ï„Î­Î»ÎµÏƒÎ¼Î± Î¼Îµ join
# --------------------------

# Î•Ï†Î±ÏÎ¼ÏŒÎ¶Î¿Ï…Î¼Îµ flatMapValues Î³Î¹Î± Î½Î± "Î¾ÎµÎ´Î¹Ï€Î»ÏÏƒÎ¿Ï…Î¼Îµ" Ï„Î± Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î±
joined = grouped.flatMapValues(lambda x: arrange(x))

# Î•Ï€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹: (department_id, ((employee_id, name, dept_id), (dept_id, dept_name)))
for record in joined.collect():
    print(record)
```


## Dataframes

ÎˆÎ½Î± **DataFrame** ÎµÎ¯Î½Î±Î¹ Î¼Î¹Î± **ÎºÎ±Ï„Î±Î½ÎµÎ¼Î·Î¼Î­Î½Î· ÏƒÏ…Î»Î»Î¿Î³Î® Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ Î¿ÏÎ³Î±Î½Ï‰Î¼Î­Î½Î· ÏƒÎµ Î¿Î½Î¿Î¼Î±Ï„Î¹ÏƒÎ¼Î­Î½ÎµÏ‚ ÏƒÏ„Î®Î»ÎµÏ‚**. Î•Î½Î½Î¿Î¹Î¿Î»Î¿Î³Î¹ÎºÎ¬ Î¹ÏƒÎ¿Î´Ï…Î½Î±Î¼ÎµÎ¯ Î¼Îµ Î­Î½Î±Î½ Ï€Î¯Î½Î±ÎºÎ± ÏƒÎµ ÏƒÏ‡ÎµÏƒÎ¹Î±ÎºÎ® Î²Î¬ÏƒÎ· Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ Î® Î¼Îµ Î­Î½Î± data frame ÏƒÎµ R/Python, Î±Î»Î»Î¬ Î¼Îµ Ï€Î¹Î¿ Î¹ÏƒÏ‡Ï…ÏÎ­Ï‚ Î²ÎµÎ»Ï„Î¹ÏƒÏ„Î¿Ï€Î¿Î¹Î®ÏƒÎµÎ¹Ï‚ "ÎºÎ¬Ï„Ï‰ Î±Ï€ÏŒ Ï„Î¿ ÎºÎ±Ï€ÏŒ".

Î¤Î± DataFrames Î¼Ï€Î¿ÏÎ¿ÏÎ½ Î½Î± Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î·Î¸Î¿ÏÎ½ Î±Ï€ÏŒ **Ï€Î¿Î¹ÎºÎ¹Î»Î¯Î± Ï€Î·Î³ÏÎ½**, ÏŒÏ€Ï‰Ï‚:

- Î´Î¿Î¼Î·Î¼Î­Î½Î± Î±ÏÏ‡ÎµÎ¯Î± Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ (Ï€.Ï‡. CSV, JSON),
- Ï€Î¯Î½Î±ÎºÎµÏ‚ Ï„Î¿Ï… Hive,
- ÎµÎ¾Ï‰Ï„ÎµÏÎ¹ÎºÎ­Ï‚ Î²Î¬ÏƒÎµÎ¹Ï‚ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½,
Ï…Ï€Î¬ÏÏ‡Î¿Î½Ï„Î± RDDs.

Î¤Î¿ API Ï„Ï‰Î½ DataFrames ÎµÎ¯Î½Î±Î¹ Î´Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÏŒ Î±Ï€ÏŒ Ï„Î¿ API Ï„Ï‰Î½ RDDs. Î— [Ï„ÎµÎºÎ¼Î·ÏÎ¯Ï‰ÏƒÎ· Ï„Î¿Ï… API Î²ÏÎ¯ÏƒÎºÎµÏ„Î±Î¹ ÎµÎ´Ï](  https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html).

Î ÏÏÏ„Î±, Î±Ï‚ Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î®ÏƒÎ¿Ï…Î¼Îµ Î­Î½Î± Ï€ÏÏŒÎ³ÏÎ±Î¼Î¼Î± Î³Î¹Î± Ï„Î¿ **Î•ÏÏÏ„Î·Î¼Î± 1**

Î“Î¹Î± Î½Î± ÎµÎºÏ„ÎµÎ»Î­ÏƒÎµÏ„Îµ Ï„Î¿ Ï€ÏÏŒÎ³ÏÎ±Î¼Î¼Î± **DFQ1.py**, Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î®ÏƒÏ„Îµ Ï„Î·Î½ Ï€Î±ÏÎ±ÎºÎ¬Ï„Ï‰ ÎµÎ½Ï„Î¿Î»Î®:

```bash
# âš ï¸ Î‘Î½Ï„Î¹ÎºÎ±Ï„Î­ÏƒÏ„Î·ÏƒÎµ Ï„Î¿ "ikons" Î¼Îµ Ï„Î¿ Î´Î¹ÎºÏŒ ÏƒÎ¿Ï… ğŸ‘‡ username
spark-submit hdfs://hdfs-namenode:9000/user/ikons/code/DFQ1.py
```

DFQ1.py:

```python
from pyspark.sql import SparkSession
from pyspark.sql.types import StructField, StructType, IntegerType, FloatType, StringType
from pyspark.sql.functions import col

# âš ï¸ Î‘Î½Ï„Î¹ÎºÎ±Ï„Î­ÏƒÏ„Î·ÏƒÎµ ğŸ‘‡ Ï„Î¿ "ikons" Î¼Îµ Ï„Î¿ Î´Î¹ÎºÏŒ ÏƒÎ¿Ï… username
username = "ikons"
spark = SparkSession \
    .builder \
    .appName("DF query 1 execution") \
    .getOrCreate()
sc = spark.sparkContext

# Î•Î›Î‘Î§Î™Î£Î¤ÎŸÎ ÎŸÎ™Î—Î£Î— Î•ÎÎŸÎ”Î©Î ÎšÎ‘Î¤Î‘Î“Î¡Î‘Î¦Î—Î£ (LOGGING)
sc.setLogLevel("ERROR")

job_id = spark.sparkContext.applicationId
output_dir = f"hdfs://hdfs-namenode:9000/user/{username}/DFQ1_{job_id}"

# ÎŸÏÎ¹ÏƒÎ¼ÏŒÏ‚ ÏƒÏ‡Î®Î¼Î±Ï„Î¿Ï‚ Î³Î¹Î± Ï„Î¿ DataFrame Ï„Ï‰Î½ Ï…Ï€Î±Î»Î»Î®Î»Ï‰Î½
employees_schema = StructType([
    StructField("id", IntegerType()),
    StructField("name", StringType()),
    StructField("salary", FloatType()),
    StructField("dep_id", IntegerType()),
])

# Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Ï„Î¿Ï… DataFrame Ï„Ï‰Î½ Ï…Ï€Î±Î»Î»Î®Î»Ï‰Î½
employees_df = spark.read.format('csv') \
    .options(header='false') \
    .schema(employees_schema) \
    .load(f"hdfs://hdfs-namenode:9000/user/{username}/examples/employees.csv")

# Î¤Î±Î¾Î¹Î½ÏŒÎ¼Î·ÏƒÎ· Ï„Ï‰Î½ Ï…Ï€Î±Î»Î»Î®Î»Ï‰Î½ Î²Î¬ÏƒÎµÎ¹ Î¼Î¹ÏƒÎ¸Î¿Ï
sorted_employees_df = employees_df.sort(col("salary"))

# Î•Î¼Ï†Î¬Î½Î¹ÏƒÎ· Ï„Ï‰Î½ Ï„Î±Î¾Î¹Î½Î¿Î¼Î·Î¼Î­Î½Ï‰Î½ Ï…Ï€Î±Î»Î»Î®Î»Ï‰Î½ (Î³Î¹Î± Î´Î¿ÎºÎ¹Î¼Î±ÏƒÏ„Î¹ÎºÎ¿ÏÏ‚ ÏƒÎºÎ¿Ï€Î¿ÏÏ‚)
sorted_employees_df.show(5)

# Î£Ï…Î³Ï‡ÏÎ½ÎµÏ…ÏƒÎ· Ï„Ï‰Î½ partitions ÏƒÎµ Î­Î½Î± ÎºÎ±Î¹ Î±Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· ÏƒÏ„Î¿ HDFS
sorted_employees_df.coalesce(1).write.format("csv").option("header", "false").save(output_dir)
```

Î•Ï€ÎµÎ¹Î´Î® **Î´ÎµÎ½ Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ RDDs**, Ï„Î¿ Î¼ÏŒÎ½Î¿ Ï€Î¿Ï… Ï‡ÏÎµÎ¹Î¬Î¶ÎµÏ„Î±Î¹ ÎµÎ¯Î½Î±Î¹ Î½Î± Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î®ÏƒÎ¿Ï…Î¼Îµ Î­Î½Î± **SparkSession** Î³Î¹Î± Î½Î± Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î®ÏƒÎ¿Ï…Î¼Îµ DataFrames. ÎšÎ±Î¸ÏÏ‚ Ï„Î¿ Î±ÏÏ‡ÎµÎ¯Î¿ **employees.csv Î´ÎµÎ½ Ï€ÎµÏÎ¹Î­Ï‡ÎµÎ¹ ÏƒÏ‡Î®Î¼Î±** (Î¿Î½ÏŒÎ¼Î±Ï„Î± ÏƒÏ„Î·Î»ÏÎ½), Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î¿ÏÎ¼Îµ ÎµÎ¼ÎµÎ¯Ï‚ Ï„Î¿ **schema**** Î³Î¹Î± Ï„Î¿ DataFrame Ï„Ï‰Î½ Ï…Ï€Î±Î»Î»Î®Î»Ï‰Î½** ÎºÎ±Î¹ Î´Î¹Î±Î²Î¬Î¶Î¿Ï…Î¼Îµ Ï„Î± Î±ÏÏ‡ÎµÎ¯Î± .csv Î±Ï€ÏŒ Ï„Î¿ HDFS. (Î¦ÏÎ¿Î½Ï„Î¯Î¶Î¿Ï…Î¼Îµ Î½Î± Ï€ÎµÏÎ¬ÏƒÎ¿Ï…Î¼Îµ Ï„Î¿ `employees_schema` ÏƒÏ„Î· ÏƒÏ…Î½Î¬ÏÏ„Î·ÏƒÎ· `schema()`.) Î£Ï„Î· ÏƒÏ…Î½Î­Ï‡ÎµÎ¹Î±, Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ Ï„Î·Î½ ÎµÎ½ÏƒÏ‰Î¼Î±Ï„Ï‰Î¼Î­Î½Î· ÏƒÏ…Î½Î¬ÏÏ„Î·ÏƒÎ· `sort()` Ï„Î¿Ï… DataFrame ÎºÎ±Î¹ Ï‰Ï‚ ÏŒÏÎ¹ÏƒÎ¼Î± Î´Î¯Î½Î¿Ï…Î¼Îµ Ï„Î· ÏƒÏ…Î½Î¬ÏÏ„Î·ÏƒÎ· `col("salary")`, Î· Î¿Ï€Î¿Î¯Î± ÎµÏ€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹ Ï„Î· ÏƒÏ„Î®Î»Î· **salary** Ï„Î¿Ï… DataFrame. Î¤Î­Î»Î¿Ï‚, ÎµÎºÏ„Ï…Ï€ÏÎ½Î¿Ï…Î¼Îµ Ï„Î¹Ï‚ **5 Ï€ÏÏÏ„ÎµÏ‚ ÎµÎ³Î³ÏÎ±Ï†Î­Ï‚** Î¼Îµ Ï„Î· ÏƒÏ…Î½Î¬ÏÏ„Î·ÏƒÎ· `show(5)`.

Î Î±ÏÎ¬Î´ÎµÎ¹Î³Î¼Î± ÎµÎ¾ÏŒÎ´Î¿Ï…:

```
+---+---------+--------+-------+
| id|    name | salary |dep_id|
+---+---------+--------+-------+
| 6 | Jerry L |  550.0 |   3   |
| 2 | John K  | 1000.0 |   2   |
| 7 |Marios K | 1000.0 |   1   |
| 5 | Helen K | 1050.0 |   2   |
|10 |Yiannis T| 1500.0 |   1   |
+---+---------+--------+-------+
```
Î“Î¹Î± Ï„Î¿ ÎµÏÏÏ„Î·Î¼Î± 2:

Î“Î¹Î± Î½Î± ÎµÎºÏ„ÎµÎ»Î­ÏƒÎµÏ„Îµ Ï„Î¿ Ï€ÏÏŒÎ³ÏÎ±Î¼Î¼Î± **DFQ2.py**, Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î®ÏƒÏ„Îµ Ï„Î·Î½ Ï€Î±ÏÎ±ÎºÎ¬Ï„Ï‰ ÎµÎ½Ï„Î¿Î»Î®:

```bash
# âš ï¸ Î‘Î½Ï„Î¹ÎºÎ±Ï„Î­ÏƒÏ„Î·ÏƒÎµ Ï„Î¿ "ikons" ğŸ‘‡ Î¼Îµ Ï„Î¿ Î´Î¹ÎºÏŒ ÏƒÎ¿Ï… username
spark-submit hdfs://hdfs-namenode:9000/user/ikons/code/DFQ2.py
```

DFQ2.py:

```python
from pyspark.sql import SparkSession
from pyspark.sql.types import StructField, StructType, IntegerType, FloatType, StringType

# âš ï¸ Î‘Î½Ï„Î¹ÎºÎ±Ï„Î­ÏƒÏ„Î·ÏƒÎµ ğŸ‘‡ Ï„Î¿ "ikons" Î¼Îµ Ï„Î¿ Î´Î¹ÎºÏŒ ÏƒÎ¿Ï… username
username = "ikons"
spark = SparkSession \
    .builder \
    .appName("DF query 2 execution") \
    .getOrCreate()
sc = spark.sparkContext

# Î•Î›Î‘Î§Î™Î£Î¤ÎŸÎ ÎŸÎ™Î—Î£Î— Î•ÎÎŸÎ”Î©Î ÎšÎ‘Î¤Î‘Î“Î¡Î‘Î¦Î—Î£ (LOGGING)
sc.setLogLevel("ERROR")

job_id = spark.sparkContext.applicationId
output_dir = f"hdfs://hdfs-namenode:9000/user/{username}/DFQ2_{job_id}"

# ÎŸÏÎ¹ÏƒÎ¼ÏŒÏ‚ ÏƒÏ‡Î®Î¼Î±Ï„Î¿Ï‚ Î³Î¹Î± Ï„Î¿ DataFrame Ï„Ï‰Î½ Ï…Ï€Î±Î»Î»Î®Î»Ï‰Î½
employees_schema = StructType([
    StructField("id", IntegerType()),
    StructField("name", StringType()),
    StructField("salary", FloatType()),
    StructField("dep_id", IntegerType()),
])

# Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Ï„Î¿Ï… DataFrame Ï„Ï‰Î½ Ï…Ï€Î±Î»Î»Î®Î»Ï‰Î½
employees_df = spark.read.format('csv') \
    .options(header='false') \
    .schema(employees_schema) \
    .load(f"hdfs://hdfs-namenode:9000/user/{username}/examples/employees.csv")

# ÎŸÏÎ¹ÏƒÎ¼ÏŒÏ‚ ÏƒÏ‡Î®Î¼Î±Ï„Î¿Ï‚ Î³Î¹Î± Ï„Î¿ DataFrame Ï„Ï‰Î½ Ï„Î¼Î·Î¼Î¬Ï„Ï‰Î½
departments_schema = StructType([
    StructField("id", IntegerType()),
    StructField("name", StringType()),
])

# Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Ï„Î¿Ï… DataFrame Ï„Ï‰Î½ Ï„Î¼Î·Î¼Î¬Ï„Ï‰Î½
departments_df = spark.read.format('csv') \
    .options(header='false') \
    .schema(departments_schema) \
    .load(f"hdfs://hdfs-namenode:9000/user/{username}/examples/departments.csv")

# ÎšÎ±Ï„Î±Ï‡ÏÏÎ·ÏƒÎ· Ï„Ï‰Î½ DataFrames Ï‰Ï‚ Ï€ÏÎ¿ÏƒÏ‰ÏÎ¹Î½Î¿Î¯ Ï€Î¯Î½Î±ÎºÎµÏ‚ (temporary views)
employees_df.createOrReplaceTempView("employees")
departments_df.createOrReplaceTempView("departments")

# Î•ÏÏÏ„Î·Î¼Î± Î³Î¹Î± Ï„Î·Î½ ÎµÏÏÎµÏƒÎ· Ï„Î¿Ï… id Ï„Î¿Ï… 'Dep A'
id_query = "SELECT departments.id, departments.name FROM departments WHERE depart-ments.name == 'Dep A'"
depA_id = spark.sql(id_query)
depA_id.createOrReplaceTempView("depA")

# Î•ÏÏÏ„Î·Î¼Î± Î¼Îµ ÎµÏƒÏ‰Ï„ÎµÏÎ¹ÎºÎ® ÏƒÏ…Î½Î­Î½Ï‰ÏƒÎ· (inner join) Î³Î¹Î± Ï„Î·Î½ ÎµÎ¾Î±Î³Ï‰Î³Î® Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ Ï…Ï€Î±Î»Î»Î®Î»Ï‰Î½ Ï„Î¿Ï… 'Dep A'
inner_join_query = """
    SELECT employees.name, employees.salary
    FROM employees
    INNER JOIN depA ON employees.dep_id == depA.id
    ORDER BY employees.salary DESC
"""
joined_data = spark.sql(inner_join_query)

# Î•Î¼Ï†Î¬Î½Î¹ÏƒÎ· Ï„Ï‰Î½ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ Ï„Î·Ï‚ ÏƒÏ…Î½Î­Î½Ï‰ÏƒÎ·Ï‚ (Î³Î¹Î± Î­Î»ÎµÎ³Ï‡Î¿)
joined_data.show()

# Î£Ï…Î³Ï‡ÏÎ½ÎµÏ…ÏƒÎ· ÏƒÎµ Î­Î½Î± Î¼ÏŒÎ½Î¿ partition ÎºÎ±Î¹ Î±Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· Ï„Î¿Ï… Ï„ÎµÎ»Î¹ÎºÎ¿Ï DataFrame ÏƒÏ„Î¿ HDFS
joined_data.coalesce(1).write.format("csv").option("header", "false").save(output_dir)
```

Î‘ÏÏ‡Î¹ÎºÎ¬, Î¿ÏÎ¯Î¶Î¿Ï…Î¼Îµ Ï„Î± **schemas ÎºÎ±Î¹ Î³Î¹Î± Ï„Î± Î´ÏÎ¿ Î±ÏÏ‡ÎµÎ¯Î±** ÎºÎ±Î¹ ÏƒÏ„Î· ÏƒÏ…Î½Î­Ï‡ÎµÎ¹Î± Ï„Î± Î´Î¹Î±Î²Î¬Î¶Î¿Ï…Î¼Îµ Î±Ï€ÏŒ Ï„Î¿ HDFS. ÎˆÏ€ÎµÎ¹Ï„Î±, Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ Ï„Î· ÏƒÏ…Î½Î¬ÏÏ„Î·ÏƒÎ· `registerTempTable` Î® `createOrReplaceTempView`, Î· Î¿Ï€Î¿Î¯Î± Î¼Î±Ï‚ ÎµÏ€Î¹Ï„ÏÎ­Ï€ÎµÎ¹ Î½Î± Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ Ï„Î± **DataFrames ÏƒÎ±Î½ SQL Ï€Î¯Î½Î±ÎºÎµÏ‚**. ÎˆÏ„ÏƒÎ¹, Î¼Ï€Î¿ÏÎ¿ÏÎ¼Îµ Î½Î± Î±Î½Î±Ï†ÎµÏÏŒÎ¼Î±ÏƒÏ„Îµ ÏƒÏ„Î¿ DataFrame Ï„Ï‰Î½ Ï…Ï€Î±Î»Î»Î®Î»Ï‰Î½ Î¼Î­ÏƒÎ± ÏƒÎµ SQL queries Ï‰Ï‚ employees ÎºÎ±Î¹ ÏƒÏ„Î¿ DataFrame Ï„Ï‰Î½ Ï„Î¼Î·Î¼Î¬Ï„Ï‰Î½ Ï‰Ï‚ departments. Î£Ï„Î· ÏƒÏ…Î½Î­Ï‡ÎµÎ¹Î±, ÎµÎºÏ„ÎµÎ»Î¿ÏÎ¼Îµ Î­Î½Î± SQL query Î³Î¹Î± Î½Î± Î²ÏÎ¿ÏÎ¼Îµ Ï„Î¿ **ID Ï„Î¿Ï… Ï„Î¼Î®Î¼Î±Ï„Î¿Ï‚ â€œDep Aâ€**. Î¤Î¿ ÎºÎ¬Î½Î¿Ï…Î¼Îµ Î±Ï…Ï„ÏŒ Î´Î·Î¼Î¹Î¿Ï…ÏÎ³ÏÎ½Ï„Î±Ï‚ Î¼Î¹Î± ÏƒÏ…Î¼Î²Î¿Î»Î¿ÏƒÎµÎ¹ÏÎ¬ Ï€Î¿Ï… Ï€ÎµÏÎ¹Î­Ï‡ÎµÎ¹ Ï„Î¿ ÎµÏÏÏ„Î·Î¼Î± ÎºÎ±Î¹ Ï„Î·Î½ ÎµÎºÏ„ÎµÎ»Î¿ÏÎ¼Îµ Î¼Îµ Ï„Î·Î½ ÎµÎ½Ï„Î¿Î»Î®:

```python
spark.sql(query)
```
Î‘Ï…Ï„ÏŒ Î´Î·Î¼Î¹Î¿Ï…ÏÎ³ÎµÎ¯ Î­Î½Î± Î½Î­Î¿ DataFrame Î¼Îµ Î¼Î¯Î± ÎµÎ³Î³ÏÎ±Ï†Î® Ï€Î¿Ï… Ï€ÎµÏÎ¹Î­Ï‡ÎµÎ¹ Ï„Î¹Ï‚ Ï„Î¹Î¼Î­Ï‚ **1 ÎºÎ±Î¹ Dep A**. ÎˆÏ€ÎµÎ¹Ï„Î±, ÎºÎ±Ï„Î±Ï‡Ï‰ÏÎ¿ÏÎ¼Îµ Ï„Î¿ DataFrame `depA` Ï‰Ï‚ Ï€ÏÎ¿ÏƒÏ‰ÏÎ¹Î½ÏŒ SQL Ï€Î¯Î½Î±ÎºÎ± Î¼Îµ Ï„Î· ÏƒÏ…Î½Î¬ÏÏ„Î·ÏƒÎ· `registerTempTable`. Î¤Î­Î»Î¿Ï‚, Î¿ÏÎ¯Î¶Î¿Ï…Î¼Îµ Ï„Î¿ SQL join query, Ï„Î¿ ÎµÎºÏ„ÎµÎ»Î¿ÏÎ¼Îµ ÎºÎ±Î¹ ÎµÎºÏ„Ï…Ï€ÏÎ½Î¿Ï…Î¼Îµ Ï„Î¹Ï‚ ÎµÎ³Î³ÏÎ±Ï†Î­Ï‚ Î¼Îµ `show()`.

**Î Î±ÏÎ¬Î´ÎµÎ¹Î³Î¼Î± ÎµÎ¾ÏŒÎ´Î¿Ï…:**

```
+---------+------+
|     name|salary|
+---------+------+
|   Mary T|2100.0|
| George T|2100.0|
| George R|2000.0|
|Yiannis T|1500.0|
| Marios K|1000.0|
+---------+------+
```

Î¥Ï€Î¬ÏÏ‡ÎµÎ¹ ÎºÎ±Î¹ Î¼Î¯Î± **ÎµÎ½Î±Î»Î»Î±ÎºÏ„Î¹ÎºÎ® ÎµÎºÏ„Î­Î»ÎµÏƒÎ· Ï„Î¿Ï… Î•ÏÏ‰Ï„Î®Î¼Î±Ï„Î¿Ï‚ 2** Î¼Îµ Ï‡ÏÎ®ÏƒÎ· **DataFrames**, Î· Î¿Ï€Î¿Î¯Î± **Î´ÎµÎ½ Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯ ÎµÎ½Î´Î¹Î¬Î¼ÎµÏƒÎ¿ Ï€Î¯Î½Î±ÎºÎ±**:

Î“Î¹Î± Î½Î± ÎµÎºÏ„ÎµÎ»Î­ÏƒÎµÏ„Îµ Ï„Î¿ Ï€ÏÏŒÎ³ÏÎ±Î¼Î¼Î± `DFQ2_noI.py`, Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î®ÏƒÏ„Îµ Ï„Î·Î½ ÎµÎ¾Î®Ï‚ ÎµÎ½Ï„Î¿Î»Î®:

```bash
# âš ï¸ Î‘Î½Ï„Î¹ÎºÎ±Ï„Î­ÏƒÏ„Î·ÏƒÎµ Ï„Î¿ "ikons" Î¼Îµ Ï„Î¿ Î´Î¹ÎºÏŒ ÏƒÎ¿Ï… ğŸ‘‡ username
spark-submit hdfs://hdfs-namenode:9000/user/ikons/code/DFQ2_noI.py
```

DFQ2_noI.py:

```python
from pyspark.sql import SparkSession
from pyspark.sql.types import StructField, StructType, IntegerType, FloatType, StringType

# âš ï¸ Î‘Î½Ï„Î¹ÎºÎ±Ï„Î­ÏƒÏ„Î·ÏƒÎµ Ï„Î¿ "ikons" ğŸ‘‡ Î¼Îµ Ï„Î¿ Î´Î¹ÎºÏŒ ÏƒÎ¿Ï… username
username = "ikons"
spark = SparkSession \
    .builder \
    .appName("DF query 2 execution, no Intermediate Table") \
    .getOrCreate()
sc = spark.sparkContext

# Î•Î›Î‘Î§Î™Î£Î¤ÎŸÎ ÎŸÎ™Î—Î£Î— Î•ÎÎŸÎ”Î©Î ÎšÎ‘Î¤Î‘Î“Î¡Î‘Î¦Î—Î£ (LOGGING)
sc.setLogLevel("ERROR")

job_id = spark.sparkContext.applicationId
output_dir = f"hdfs://hdfs-namenode:9000/user/{username}/DFQ2_nol_{job_id}"


# ÎŸÏÎ¹ÏƒÎ¼ÏŒÏ‚ ÏƒÏ‡Î®Î¼Î±Ï„Î¿Ï‚ Î³Î¹Î± Ï„Î¿ DataFrame Ï„Ï‰Î½ Ï…Ï€Î±Î»Î»Î®Î»Ï‰Î½
employees_schema = StructType([
    StructField("id", IntegerType()),
    StructField("name", StringType()),
    StructField("salary", FloatType()),
    StructField("dep_id", IntegerType()),
])

# Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Ï„Î¿Ï… DataFrame Ï„Ï‰Î½ Ï…Ï€Î±Î»Î»Î®Î»Ï‰Î½
employees_df = spark.read.format('csv') \
    .options(header='false') \
    .schema(employees_schema) \
    .load(f"hdfs://hdfs-namenode:9000/user/{username}/examples/employees.csv")

# ÎŸÏÎ¹ÏƒÎ¼ÏŒÏ‚ ÏƒÏ‡Î®Î¼Î±Ï„Î¿Ï‚ Î³Î¹Î± Ï„Î¿ DataFrame Ï„Ï‰Î½ Ï„Î¼Î·Î¼Î¬Ï„Ï‰Î½
departments_schema = StructType([
    StructField("id", IntegerType()),
    StructField("name", StringType()),
])

# Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Ï„Î¿Ï… DataFrame Ï„Ï‰Î½ Ï„Î¼Î·Î¼Î¬Ï„Ï‰Î½
departments_df = spark.read.format('csv') \
    .options(header='false') \
    .schema(departments_schema) \
    .load(f"hdfs://hdfs-namenode:9000/user/{username}/examples/departments.csv")

# ÎšÎ±Ï„Î±Ï‡ÏÏÎ·ÏƒÎ· Ï„Ï‰Î½ DataFrames Ï‰Ï‚ Ï€ÏÎ¿ÏƒÏ‰ÏÎ¹Î½Î¿Î¯ Ï€Î¯Î½Î±ÎºÎµÏ‚ (temporary views)
employees_df.createOrReplaceTempView("employees")
departments_df.createOrReplaceTempView("departments")

# Î•ÏÏÏ„Î·Î¼Î± Î¼Îµ ÎµÏƒÏ‰Ï„ÎµÏÎ¹ÎºÎ® ÏƒÏ…Î½Î­Î½Ï‰ÏƒÎ· (inner join) Î³Î¹Î± Ï„Î·Î½ ÎµÎ¾Î±Î³Ï‰Î³Î® Ï…Ï€Î±Î»Î»Î®Î»Ï‰Î½ Ï„Î¿Ï… 'Dep A'
inner_join_query = """
    SELECT employees.name, employees.salary
    FROM employees
    INNER JOIN departments ON employees.dep_id == departments.id
    WHERE departments.name == 'Dep A'
    ORDER BY employees.salary DESC
"""

# Î•ÎºÏ„Î­Î»ÎµÏƒÎ· Ï„Î¿Ï… ÎµÏÏ‰Ï„Î®Î¼Î±Ï„Î¿Ï‚
joined_data = spark.sql(inner_join_query)

# Î•Î¼Ï†Î¬Î½Î¹ÏƒÎ· Ï„Ï‰Î½ Î±Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î¬Ï„Ï‰Î½ Ï„Î·Ï‚ ÏƒÏÎ½Î´ÎµÏƒÎ·Ï‚ (Î³Î¹Î± Î­Î»ÎµÎ³Ï‡Î¿)
joined_data.show()

# Î£Ï…Î³Ï‡ÏÎ½ÎµÏ…ÏƒÎ· Ï„Ï‰Î½ partitions ÏƒÎµ Î­Î½Î± ÎºÎ±Î¹ Î±Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· ÏƒÏ„Î¿ HDFS
joined_data.coalesce(1).write.format("csv").option("header", "false").save(output_dir)
```

ÎœÏ€Î¿ÏÎ¿ÏÎ¼Îµ Î½Î± ÎµÎºÏ„ÎµÎ»Î­ÏƒÎ¿Ï…Î¼Îµ **ÎµÎ½ÏÏƒÎµÎ¹Ï‚ (join)** ÏƒÏ„Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î± Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÏÎ½Ï„Î±Ï‚ **Î¼ÏŒÎ½Î¿ Ï„Î¹Ï‚ ÎµÎ½ÏƒÏ‰Î¼Î±Ï„Ï‰Î¼Î­Î½ÎµÏ‚ ÏƒÏ…Î½Î±ÏÏ„Î®ÏƒÎµÎ¹Ï‚ Ï„Ï‰Î½ DataFrames**. Î“Î¹Î± Ï€Î±ÏÎ¬Î´ÎµÎ¹Î³Î¼Î±, Î³Î¹Î± Î½Î± Ï…Ï€Î¿Î»Î¿Î³Î¯ÏƒÎ¿Ï…Î¼Îµ Ï„Î¿ **Î¬Î¸ÏÎ¿Î¹ÏƒÎ¼Î± Ï„Ï‰Î½ Î¼Î¹ÏƒÎ¸ÏÎ½ Î±Î½Î¬ Ï„Î¼Î®Î¼Î±**, Î¼Ï€Î¿ÏÎ¿ÏÎ¼Îµ Î½Î± Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î®ÏƒÎ¿Ï…Î¼Îµ Ï„Î¿ Ï€Î±ÏÎ±ÎºÎ¬Ï„Ï‰ Ï€ÏÏŒÎ³ÏÎ±Î¼Î¼Î±. Î“Î¹Î± Î½Î± ÎµÎºÏ„ÎµÎ»Î­ÏƒÎµÏ„Îµ Ï„Î¿ Ï€ÏÏŒÎ³ÏÎ±Î¼Î¼Î± **DF2b.py**, Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î®ÏƒÏ„Îµ Ï„Î·Î½ ÎµÎ¾Î®Ï‚ ÎµÎ½Ï„Î¿Î»Î®:


```bash
# âš ï¸ Î‘Î½Ï„Î¹ÎºÎ±Ï„Î­ÏƒÏ„Î·ÏƒÎµ Ï„Î¿ "ikons" Î¼Îµ Ï„Î¿ Î´Î¹ÎºÏŒ ÏƒÎ¿Ï… ğŸ‘‡ username
spark-submit hdfs://hdfs-namenode:9000/user/ikons/code/DF2b.py
```

DF2b.py:

```python
from pyspark.sql import SparkSession
from pyspark.sql.types import StructField, StructType, IntegerType, FloatType, StringType

# âš ï¸ Î‘Î½Ï„Î¹ÎºÎ±Ï„Î­ÏƒÏ„Î·ÏƒÎµ Ï„Î¿ "ikons" ğŸ‘‡ Î¼Îµ Ï„Î¿ Î´Î¹ÎºÏŒ ÏƒÎ¿Ï… username
username = "ikons"
spark = SparkSession \
    .builder \
    .appName("DF query 2b execution") \
    .getOrCreate()
sc = spark.sparkContext
# Î•Î›Î‘Î§Î™Î£Î¤ÎŸÎ ÎŸÎ™Î—Î£Î— Î•ÎÎŸÎ”Î©Î ÎšÎ‘Î¤Î‘Î“Î¡Î‘Î¦Î—Î£ (LOGGING)
sc.setLogLevel("ERROR")

job_id = spark.sparkContext.applicationId
output_dir = f"hdfs://hdfs-namenode:9000/user/{username}/DF2b_{job_id}"

# ÎŸÏÎ¹ÏƒÎ¼ÏŒÏ‚ ÏƒÏ‡Î®Î¼Î±Ï„Î¿Ï‚ Î³Î¹Î± Ï„Î¿ DataFrame Ï„Ï‰Î½ Ï…Ï€Î±Î»Î»Î®Î»Ï‰Î½
employees_schema = StructType([
    StructField("emp_id", IntegerType()),
    StructField("emp_name", StringType()),
    StructField("salary", FloatType()),
    StructField("dep_id", IntegerType()),
])

# Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Ï„Î¿Ï… DataFrame Ï„Ï‰Î½ Ï…Ï€Î±Î»Î»Î®Î»Ï‰Î½
employees_df = spark.read.format('csv') \
    .options(header='false') \
    .schema(employees_schema) \
    .load(f"hdfs://hdfs-namenode:9000/user/{username}/examples/employees.csv")

# ÎŸÏÎ¹ÏƒÎ¼ÏŒÏ‚ ÏƒÏ‡Î®Î¼Î±Ï„Î¿Ï‚ Î³Î¹Î± Ï„Î¿ DataFrame Ï„Ï‰Î½ Ï„Î¼Î·Î¼Î¬Ï„Ï‰Î½
departments_schema = StructType([
    StructField("id", IntegerType()),
    StructField("dpt_name", StringType()),
])

# Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Ï„Î¿Ï… DataFrame Ï„Ï‰Î½ Ï„Î¼Î·Î¼Î¬Ï„Ï‰Î½
departments_df = spark.read.format('csv') \
    .options(header='false') \
    .schema(departments_schema) \
    .load(f"hdfs://hdfs-namenode:9000/user/{username}/examples/departments.csv")

# Î•ÎºÏ„Î­Î»ÎµÏƒÎ· ÎµÏƒÏ‰Ï„ÎµÏÎ¹ÎºÎ®Ï‚ ÏƒÏÎ½Î´ÎµÏƒÎ·Ï‚ (inner join) Î¼ÎµÏ„Î±Î¾Ï Ï„Ï‰Î½ DataFrames Ï…Ï€Î±Î»Î»Î®Î»Ï‰Î½ ÎºÎ±Î¹ Ï„Î¼Î·Î¼Î¬Ï„Ï‰Î½
joinedDf = employees_df.join(departments_df, employees_df.dep_id == departments_df.id, "inner")

# Î•Î¼Ï†Î¬Î½Î¹ÏƒÎ· Ï„Ï‰Î½ ÏƒÏ…Î½Î´ÎµÎ´ÎµÎ¼Î­Î½Ï‰Î½ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ (Î³Î¹Î± Î­Î»ÎµÎ³Ï‡Î¿)
joinedDf.show()

# ÎŸÎ¼Î±Î´Î¿Ï€Î¿Î¯Î·ÏƒÎ· ÎºÎ±Ï„Î¬ Î±Î½Î±Î³Î½Ï‰ÏÎ¹ÏƒÏ„Î¹ÎºÏŒ Ï„Î¼Î®Î¼Î±Ï„Î¿Ï‚ ÎºÎ±Î¹ Ï…Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ Î±Î¸ÏÎ¿Î¯ÏƒÎ¼Î±Ï„Î¿Ï‚ Î¼Î¹ÏƒÎ¸ÏÎ½
groupedDf = joinedDf.groupBy("dep_id").sum("salary")

# Î•Î¼Ï†Î¬Î½Î¹ÏƒÎ· Ï„Ï‰Î½ Î¿Î¼Î±Î´Î¿Ï€Î¿Î¹Î·Î¼Î­Î½Ï‰Î½ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ (Î³Î¹Î± Î­Î»ÎµÎ³Ï‡Î¿)
groupedDf.show()

# Î£Ï…Î³Ï‡ÏÎ½ÎµÏ…ÏƒÎ· Ï„Ï‰Î½ DataFrames ÏƒÎµ Î­Î½Î± Î¼ÏŒÎ½Î¿ partition ÎºÎ±Î¹ Î±Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ® Ï„Î¿Ï…Ï‚ ÏƒÏ„Î¿ HDFS

# Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· Ï„Î¿Ï… DataFrame Ï„Î·Ï‚ ÏƒÏÎ½Î´ÎµÏƒÎ·Ï‚ ÏƒÏ„Î¿ HDFS
joinedDf.coalesce(1).write.format("csv").option("header", "false").save(f"{output_dir}_joined")

# Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· Ï„Î¿Ï… Î¿Î¼Î±Î´Î¿Ï€Î¿Î¹Î·Î¼Î­Î½Î¿Ï… DataFrame ÏƒÏ„Î¿ HDFS
groupedDf.coalesce(1).write.format("csv").option("header", "false").save(f"{output_dir}_grouped")

```

Î‘ÏÏ‡Î¹ÎºÎ¬, Î´Î¹Î±Î²Î¬Î¶Î¿Ï…Î¼Îµ Ï„Î± Î´ÏÎ¿ ÏƒÏÎ½Î¿Î»Î± Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ Î±Ï€ÏŒ Ï„Î¿ **HDFS**. Î£Ï„Î· ÏƒÏ…Î½Î­Ï‡ÎµÎ¹Î±, Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ Ï„Î·Î½ ÎµÎ½ÏƒÏ‰Î¼Î±Ï„Ï‰Î¼Î­Î½Î· ÏƒÏ…Î½Î¬ÏÏ„Î·ÏƒÎ· [`join`](  https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.join.html), ÎºÎ±Î¸Î¿ÏÎ¯Î¶Î¿Î½Ï„Î±Ï‚ Ï‰Ï‚ ÎºÎ»ÎµÎ¹Î´Î¯ Î­Î½Ï‰ÏƒÎ·Ï‚ Ï„Î¿ `dep_id` Ï„Î¿Ï… Ï…Ï€Î±Î»Î»Î®Î»Î¿Ï… (`employees_df.dep_id`) ÎºÎ±Î¹ Ï„Î¿ `id` Ï„Î¿Ï… Ï„Î¼Î®Î¼Î±Ï„Î¿Ï‚ (`departments_df.id`), ÎºÎ±Î¸ÏÏ‚ ÎºÎ±Î¹ Ï„Î¿Î½ Ï„ÏÏ€Î¿ Ï„Î·Ï‚ ÏƒÏ…Î½Î­Î½Ï‰ÏƒÎ·Ï‚ â€” ÏƒÏ„Î·Î½ Ï€ÏÎ¿ÎºÎµÎ¹Î¼Î­Î½Î· Ï€ÎµÏÎ¯Ï€Ï„Ï‰ÏƒÎ· ÎµÏ€Î¹Î»Î­Î³Î¿Ï…Î¼Îµ **"inner"**.

ÎˆÏ€ÎµÎ¹Ï„Î±, ÎºÎ¬Î½Î¿Ï…Î¼Îµ **[Î¿Î¼Î±Î´Î¿Ï€Î¿Î¯Î·ÏƒÎ· (`groupBy`)](  https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.groupBy.html)** Ï„Ï‰Î½ ÎµÎ³Î³ÏÎ±Ï†ÏÎ½ Î¼Îµ Î²Î¬ÏƒÎ· Ï„Î·Î½ Ï„Î¹Î¼Î® Ï„Î¿Ï… `dep_id`, ÎºÎ±Î¹ Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ¼Îµ Ï„Î· ÏƒÏ…Î½Î¬ÏÏ„Î·ÏƒÎ· **`[sum()](  https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/api/pyspark.pandas.DataFrame.sum.html)`** Î³Î¹Î± Î½Î± Ï…Ï€Î¿Î»Î¿Î³Î¯ÏƒÎ¿Ï…Î¼Îµ Ï„Î¿ **Î¬Î¸ÏÎ¿Î¹ÏƒÎ¼Î± Ï„Ï‰Î½ Î¼Î¹ÏƒÎ¸ÏÎ½** ÏƒÎµ ÎºÎ¬Î¸Îµ Ï„Î¼Î®Î¼Î±. Î¤Î­Î»Î¿Ï‚, ÎµÎºÏ„Ï…Ï€ÏÎ½Î¿Ï…Î¼Îµ Ï„Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î±.

```
+------+----------+------+------+---+--------+
|emp_id|  emp_name|salary|dep_id| id|dpt_name|
+------+----------+------+------+---+--------+
|     1|  George R|2000.0|     1|  1|   Dep A|
|     2|    John K|1000.0|     2|  2|   Dep B|
|     3|    Mary T|2100.0|     1|  1|   Dep A|
|     4|  George T|2100.0|     1|  1|   Dep A|
|     5|   Helen K|1050.0|     2|  2|   Dep B|
|     6|   Jerry L| 550.0|     3|  3|   Dep C|
|     7|  Marios K|1000.0|     1|  1|   Dep A|
|     8|  George K|2500.0|     2|  2|   Dep B|
|     9|Vasilios D|3500.0|     3|  3|   Dep C|
|    10| Yiannis T|1500.0|     1|  1|   Dep A|
|    11| Antonis T|2500.0|     2|  2|   Dep B|
+------+----------+------+------+---+--------+

+------+-----------+
|dep_id|sum(salary)|
+------+-----------+
|     1|     8700.0|
|     3|     4050.0|
|     2|     7050.0|
+------+-----------+
```

Î“Î¹Î± Ï„Î¿ ÎµÏÏÏ„Î·Î¼Î± 3:

Î“Î¹Î± Î½Î± ÎµÎºÏ„ÎµÎ»Î­ÏƒÎµÏ„Îµ Ï„Î¿ Ï€ÏÏŒÎ³ÏÎ±Î¼Î¼Î± `DFQ3.py`, Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î®ÏƒÏ„Îµ Ï„Î·Î½ Ï€Î±ÏÎ±ÎºÎ¬Ï„Ï‰ ÎµÎ½Ï„Î¿Î»Î®:

```bash
# âš ï¸ Î‘Î½Ï„Î¹ÎºÎ±Ï„Î­ÏƒÏ„Î·ÏƒÎµ Ï„Î¿ "ikons" Î¼Îµ Ï„Î¿ Î´Î¹ÎºÏŒ ÏƒÎ¿Ï… ğŸ‘‡ username
spark-submit hdfs://hdfs-namenode:9000/user/ikons/code/DFQ3.py
```
Î£Ï„Î¿Î½ ÎºÏÎ´Î¹ÎºÎ± Î´Î¯Î½ÎµÏ„Î±Î¹ Î­Î½Î± Ï€Î±ÏÎ¬Î´ÎµÎ¹Î³Î¼Î± Ï‡ÏÎ®ÏƒÎ·Ï‚ Ï„Ï‰Î½ [User Defined Functions (UDFs)](https://spark.apache.org/docs/latest/sql-ref-functions.html#udfs-user-defined-functions)  .

DFQ3.py

```python
from pyspark.sql import SparkSession
from pyspark.sql.types import StructField, StructType, IntegerType, FloatType, StringType
from pyspark.sql.functions import col, udf

# âš ï¸ Î‘Î½Ï„Î¹ÎºÎ±Ï„Î­ÏƒÏ„Î·ÏƒÎµ Ï„Î¿ "ikons" ğŸ‘‡ Î¼Îµ Ï„Î¿ Î´Î¹ÎºÏŒ ÏƒÎ¿Ï… username
username = "ikons"
spark = SparkSession \
    .builder \
    .appName("DF query 3 execution") \
    .getOrCreate()
sc = spark.sparkContext
# Î•Î›Î‘Î§Î™Î£Î¤ÎŸÎ ÎŸÎ™Î—Î£Î— Î•ÎÎŸÎ”Î©Î ÎšÎ‘Î¤Î‘Î“Î¡Î‘Î¦Î—Î£ (LOGGING)
sc.setLogLevel("ERROR")

job_id = spark.sparkContext.applicationId
output_dir = f"hdfs://hdfs-namenode:9000/user/{username}/DF3_{job_id}"

# ÎŸÏÎ¹ÏƒÎ¼ÏŒÏ‚ ÏƒÏ‡Î®Î¼Î±Ï„Î¿Ï‚ Î³Î¹Î± Ï„Î¿ DataFrame Ï„Ï‰Î½ Ï…Ï€Î±Î»Î»Î®Î»Ï‰Î½
employees_schema = StructType([
    StructField("emp_id", IntegerType()),
    StructField("emp_name", StringType()),
    StructField("salary", FloatType()),
    StructField("dep_id", IntegerType()),
])

# Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Ï„Î¿Ï… DataFrame Ï„Ï‰Î½ Ï…Ï€Î±Î»Î»Î®Î»Ï‰Î½
employees_df = spark.read.format('csv') \
    .options(header='false') \
    .schema(employees_schema) \
    .load(f"hdfs://hdfs-namenode:9000/user/{username}/examples/employees.csv")

# Î”Î®Î»Ï‰ÏƒÎ· ÏƒÏ…Î½Î¬ÏÏ„Î·ÏƒÎ·Ï‚ Ï…Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼Î¿Ï Ï„Î¿Ï… ÎµÏ„Î®ÏƒÎ¹Î¿Ï… ÎµÎ¹ÏƒÎ¿Î´Î®Î¼Î±Ï„Î¿Ï‚
def calculate_yearly_income(salary):
    return 14*salary
# ÎšÎ±Ï„Î±Ï‡ÏÏÎ·ÏƒÎ· Ï„Î¿Ï… udf
calculate_yearly_income_udf = udf(calculate_yearly_income, FloatType())
# Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ Î¼Îµ Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Î½Î­Î±Ï‚ ÏƒÏ„Î®Î»Î·Ï‚
employees_yearly_income_df = employees_df \
    .withColumn("yearly_income", calculate_yearly_income_udf(col("salary"))) \
    .select("emp_name", "yearly_income")
# Î•Î¼Ï†Î¬Î½Î¹ÏƒÎ· Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î¿Ï‚
employees_yearly_income_df.show()
# Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· Ï„Î¿Ï… DataFrame ÏƒÏ„Î¿ HDFS
employees_yearly_income_df.coalesce(1) \
    .write.format("csv") \
    .option("header", "false") \
    .save(f"{output_dir}")
```

Î“Î¹Î± Ï„Î·Î½ Ï…Î»Î¿Ï€Î¿Î¯Î·ÏƒÎ·, Î±ÏÏ‡Î¹ÎºÎ¬ Î¿ÏÎ¯Î¶Î¿Ï…Î¼Îµ Î¼Î¹Î± ÏƒÏ…Î½Î¬ÏÏ„Î·ÏƒÎ· Ï€Î¿Ï… Î´Î­Ï‡ÎµÏ„Î±Î¹ Ï‰Ï‚ ÏŒÏÎ¹ÏƒÎ¼Î± Î­Î½Î±Î½ Î±ÏÎ¹Î¸Î¼ÏŒ (`salary`), Ï„Î¿Î½ Ï€Î¿Î»Î»Î±Ï€Î»Î±ÏƒÎ¹Î¬Î¶ÎµÎ¹ Î¼Îµ 14 ÎºÎ±Î¹ ÎµÏ€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹ Ï„Î¿ Î±Ï€Î¿Ï„Î­Î»ÎµÏƒÎ¼Î±. Î£Ï„Î· ÏƒÏ…Î½Î­Ï‡ÎµÎ¹Î±, Î¿ÏÎ¯Î¶Î¿Ï…Î¼Îµ Ï„Î¿ Î±Î½Ï„Î¯ÏƒÏ„Î¿Î¹Ï‡Î¿ **UDF** (user defined function) Ï€Î¿Ï… Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± ÎµÏ†Î±ÏÎ¼Î¿ÏƒÏ„ÎµÎ¯ Ï€Î¬Î½Ï‰ ÏƒÏ„Î¿ **DataFrame** `employees_df`, ÎºÎ±Î¹ ÏƒÏ…Î³ÎºÎµÎºÏÎ¹Î¼Î­Î½Î± Î¼Îµ ÎµÎ¯ÏƒÎ¿Î´Î¿ Ï„Î· ÏƒÏ„Î®Î»Î· `salary`.

**Î•Î¾ÎµÏ„Î¬ÏƒÏ„Îµ Ï„Î¿ ÎµÎ¾Î®Ï‚**: Î˜Î± Î¼Ï€Î¿ÏÎ¿ÏÏƒÎµ Ï„Î¿ Î¯Î´Î¹Î¿ Î±Ï€Î¿Ï„Î­Î»ÎµÏƒÎ¼Î± Î½Î± ÎµÏ€Î¹Ï„ÎµÏ…Ï‡Î¸ÎµÎ¯ Ï‡Ï‰ÏÎ¯Ï‚ Ï„Î· Ï‡ÏÎ®ÏƒÎ· UDF; Î ÏÏ‚ ÎµÏ€Î·ÏÎµÎ¬Î¶ÎµÏ„Î±Î¹ Î· ÎµÏ€Î¯Î´Î¿ÏƒÎ· ÏƒÎµ ÎºÎ¬Î¸ÎµÎ¼Î¯Î± Î±Ï€ÏŒ Ï„Î¹Ï‚ Î´ÏÎ¿ Ï€ÎµÏÎ¹Ï€Ï„ÏÏƒÎµÎ¹Ï‚;

**Î Î±ÏÎ¬Î´ÎµÎ¹Î³Î¼Î± ÎµÎ¾ÏŒÎ´Î¿Ï…:**

```
+----------+-------------+
|  emp_name|yearly_income|
+----------+-------------+
|  George R|      28000.0|
|    John K|      14000.0|
|    Mary T|      29400.0|
|  George T|      29400.0|
|   Helen K|      14700.0|
|   Jerry L|       7700.0|
|  Marios K|      14000.0|
|  George K|      35000.0|
|Vasilios D|      49000.0|
| Yiannis T|      21000.0|
| Antonis T|      35000.0|
+----------+-------------+
```



## Î•Î³ÎºÎ±Ï„Î¬ÏƒÏ„Î±ÏƒÎ· Spark History Server Î³Î¹Î± Ï€ÏÎ¿Î²Î¿Î»Î® Î¹ÏƒÏ„Î¿ÏÎ¹ÎºÏÎ½ ÎµÎºÏ„ÎµÎ»Î­ÏƒÎµÏ‰Î½

Î“Î¹Î± Î½Î± Î´ÎµÎ¯Ï„Îµ Ï„Î± Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î± Ï„Î·Ï‚ ÎµÎºÏ„Î­Î»ÎµÏƒÎ·Ï‚ Ï„Ï‰Î½ ÎµÏÎ³Î±ÏƒÎ¹ÏÎ½ Î¼ÎµÏ„Î¬ Ï„Î¿ Ï€Î­ÏÎ±Ï‚ Ï„Î·Ï‚ ÎµÎºÏ„Î­Î»ÎµÏƒÎ®Ï‚ Ï„Î¿Ï…Ï‚ Ï€ÏÎ­Ï€ÎµÎ¹ Î½Î± ÎµÎ³ÎºÎ±Ï„Î±ÏƒÏ„Î®ÏƒÎµÏ„Îµ Ï„Î¿Î½ **Spark History Server**. ÎŸÎ¹ ÎµÏÎ³Î±ÏƒÎ¯ÎµÏ‚ ÏƒÎ±Ï‚ Î­Ï‡Î¿Ï…Î½ ÏÏ…Î¸Î¼Î¹ÏƒÏ„ÎµÎ¯ Î½Î± Î±Ï€Î¿Î¸Î·ÎºÎµÏÎ¿Ï…Î½ Ï„Î± Î±ÏÏ‡ÎµÎ¯Î± ÎºÎ±Ï„Î±Î³ÏÎ±Ï†Î®Ï‚ Ï„Î·Ï‚ ÎµÎºÏ„Î­Î»ÎµÏƒÎ®Ï‚ Ï„Î¿Ï…Ï‚ ÏƒÏ„Î¿Î½ Ï…Ï€Î¿ÎºÎ±Ï„Î¬Î»Î¿Î³Î¿ `logs` Ï„Î¿Ï… home directory ÏƒÎ±Ï‚ ÏƒÏ„Î¿ hdfs. ÎœÎ­ÏƒÏ‰ Ï„Î¿Ï… `docker` Î¸Î± ÏƒÎ·ÎºÏÏƒÎ¿Ï…Î¼Îµ Î­Î½Î±Î½ container Ï„Î¿Ï€Î¹ÎºÎ¬ ÏƒÏ„Î¿Î½ Ï…Ï€Î¿Î»Î¿Î³Î¹ÏƒÏ„Î® Î¼Î±Ï‚ Î¿ Î¿Ï€Î¿Î¯Î¿Ï‚ Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± Î´Î¹Î±Î²Î¬Î¶ÎµÎ¹ Ï„Î± logs ÎºÎ±Î¹ Î½Î± Î²Î»Î­Ï€ÎµÏ„Îµ Ï„Î¹ Î­Ï‡ÎµÏ„Îµ Ï„ÏÎ­Î¾ÎµÎ¹ ÎºÎ±Î¹ Ï€ÏŒÏ„Îµ.

1. Î¦ÏÎ¿Î½Ï„Î¯ÏƒÏ„Îµ Î½Î± Î­Ï‡ÎµÏ„Îµ ÎµÎ½ÎµÏÎ³ÏŒ Ï„Î¿ Docker Desktop. Î ÎµÏÎ¹Î·Î³Î·Î¸Î®Ï„Îµ ÏƒÏ„Î¿Î½ ÎºÎ±Ï„Î¬Î»Î¿Î³Î¿

```bash
cd ~/bigdata-uth/docker/02-lab2-spark-history-server
```
ÎºÎ±Î¹ ÎµÎºÏ„ÎµÎ»Î­ÏƒÏ„Îµ

```bash
docker build -t spark-history-server .
```

Î¼Îµ Î±Ï…Ï„ÏŒ Ï„Î¿Î½ Ï„ÏÏŒÏ€Î¿ ÎºÎ±Ï„Î±ÏƒÎºÎµÏ…Î¬Î¶ÎµÏ„Îµ Î¼Î¹Î± Ï€ÏÎ¿ÏƒÎ±ÏÎ¼Î¿ÏƒÎ¼Î­Î½Î· ÎµÎ¹ÎºÏŒÎ½Î± Docker. 

2. Î•ÎºÎºÎ¯Î½Î·ÏƒÎ· Ï„Î¿Ï… ÎºÎ¿Î½Ï„Î­Î¹Î½ÎµÏ Î¿ÏÎ¯Î¶Î¿Î½Ï„Î±Ï‚ Ï„Î¿ **ÏŒÎ½Î¿Î¼Î± Ï‡ÏÎ®ÏƒÏ„Î· (Î±Î»Î»Î¬Î¾Ï„Îµ Ï„Î¿ ikons Î¼Îµ Ï„Î¿ Î´Î¹ÎºÏŒ ÏƒÎ¿Ï… username)**

```bash
# âš ï¸ Î‘Î½Ï„Î¹ÎºÎ±Ï„Î­ÏƒÏ„Î·ÏƒÎµ ğŸ‘‡ Ï„Î¿ "ikons" Î¼Îµ Ï„Î¿ Î´Î¹ÎºÏŒ ÏƒÎ¿Ï… username

docker run -d \
  --name spark-history-server \
  -e USERNAME=ikons \
  -p 18080:18080 \
  spark-history-server
```

3. ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ Ï„Ï‰Î½ logs

Î•ÎºÏ„Î­Î»ÎµÏƒÎµ:

```bash
docker logs -f spark-history-server
```

Î¸Î± Ï€ÏÎ­Ï€ÎµÎ¹ Î½Î± Î´ÎµÎ¹Ï‚ ÎºÎ¬Ï„Î¹ ÏŒÏ€Ï‰Ï‚:

```
ÎÎµÎºÎ¹Î½Î¬ÎµÎ¹ Ï„Î¿ Spark History Server Î³Î¹Î± Ï„Î¿Î½ Ï‡ÏÎ®ÏƒÏ„Î· ikons, logs Î±Ï€ÏŒ hdfs://hdfs-namenode:9000/user/ikons/logs
```

Î Î¬Ï„Î± `Ctrl+c` Î³Î¹Î± Î½Î± Î²Î³ÎµÎ¹Ï‚ Î±Ï€ÏŒ Ï„Î·Î½ ÎµÎºÏ„ÏÏ€Ï‰ÏƒÎ· Ï„Ï‰Î½ Î±ÏÏ‡ÎµÎ¯Ï‰Î½ ÎºÎ±Ï„Î±Î³ÏÎ±Ï†Î®Ï‚

4. Î ÏÏŒÏƒÎ²Î±ÏƒÎ· ÏƒÏ„Î¿ UI

Î‘Î½Î¿Î¯Î¾Ï„Îµ Ï„Î·Î½ Ï€Î±ÏÎ±ÎºÎ¬Ï„Ï‰ Î´Î¹ÎµÏÎ¸Ï…Î½ÏƒÎ· http://localhost:18080/

ÎšÎ±Î¹ Î¸Î± Î´ÎµÎ¯Ï„Îµ Ï„Î¹Ï‚ ÎµÏÎ³Î±ÏƒÎ¯ÎµÏ‚ Ï€Î¿Ï… Î­Ï‡ÎµÏ„Îµ ÎµÎºÏ„ÎµÎ»Î­ÏƒÎµÎ¹ Î¼Î­Ï‡ÏÎ¹ Ï„ÏÏÎ±.


![Î•Î¹ÎºÏŒÎ½Î± 4](images/img4.png)

Î‘Î½ ÎºÎ¬Î½ÎµÎ¹Ï‚ ÎºÎ»Î¹Îº ÏƒÎµ Î­Î½Î± **application id**, Î¸Î± Î´ÎµÎ¹Ï‚ ÎºÎ¬Ï„Î¹ ÏƒÎ±Î½ Î±Ï…Ï„ÏŒ:

Î£Ï„Î· ÏƒÏ…Î½Î­Ï‡ÎµÎ¹Î±, Î±Î½ ÎºÎ¬Î½ÎµÎ¹Ï‚ ÎºÎ»Î¹Îº ÏƒÎµ Î­Î½Î± **job**, Î¼Ï€Î¿ÏÎµÎ¯Ï‚ Î½Î± Î´ÎµÎ¹Ï‚ Ï€ÎµÏÎ¹ÏƒÏƒÏŒÏ„ÎµÏÎµÏ‚ Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯ÎµÏ‚ Î³Î¹Î± Ï„Î· ÏƒÏ…Î³ÎºÎµÎºÏÎ¹Î¼Î­Î½Î· ÎµÏÎ³Î±ÏƒÎ¯Î±, ÏŒÏ€Ï‰Ï‚ Ï„Î¿ **Î³ÏÎ¬Ï†Î·Î¼Î± DAG**, Ï„Î¿ **Ï‡ÏÎ¿Î½Î¹ÎºÏŒ Î´Î¹Î¬Î³ÏÎ±Î¼Î¼Î± Î³ÎµÎ³Î¿Î½ÏŒÏ„Ï‰Î½** Îº.Î¬.

![Î•Î¹ÎºÏŒÎ½Î± 5](images/img5.png)

5. Î¤ÎµÏÎ¼Î±Ï„Î¹ÏƒÎ¼ÏŒÏ‚ Ï„Î¿Ï… container ÎºÎ±Î¹ ÎµÏ€Î±Î½ÎµÎºÎºÎ¯Î½Î·ÏƒÎ· Ï„Î¿Ï… container.

ÎœÎµ Ï„Î·Î½ Ï€Î±ÏÎ±ÎºÎ¬Ï„Ï‰ ÎµÎ½Ï„Î¿Î»Î® ÏƒÏ„Î±Î¼Î±Ï„Î¬ÎµÎ¹ Î· ÎµÎºÏ„Î­Î»ÎµÏƒÎ· Ï„Î¿Ï… container (Ï€Î±ÏÎ±Î¼Î­Î½ÎµÎ¹ Î¿ container Î³Î¹Î± Î½Î± Ï„Î¿Î½ Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î®ÏƒÎµÏ„Îµ Î±ÏÎ³ÏŒÏ„ÎµÏÎ±)

```bash
docker stop spark-history-server
```

ÎœÎµ Ï„Î·Î½ Ï€Î±ÏÎ±ÎºÎ¬Ï„Ï‰ ÎµÎ½Ï„Î¿Î»Î® Î¾Î±Î½Î±Î¾ÎµÎºÎ¹Î½Î¬ÎµÎ¹ Î¿ container

```bash
docker start spark-history-server
```

ÎœÎµ Ï„Î·Î½ Ï€Î±ÏÎ±ÎºÎ¬Ï„Ï‰ ÎµÎ½Ï„Î¿Î»Î® Î´Î¹Î±Î³ÏÎ¬Ï†ÎµÏ„Î±Î¹ Î¿ container ÎºÎ±Î¹ Î¸Î± Ï€ÏÎ­Ï€ÎµÎ¹ Î½Î± Ï„Î¿Î½ Î¾Î±Î½Î±Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î®ÏƒÎµÎ¹Ï‚ Î±ÎºÎ¿Î»Î¿Ï…Î¸ÏÎ½Ï„Î±Ï‚ Ï„Î± Î²Î®Î¼Î±Ï„Î± 1 ÎºÎ±Î¹ 2.

```bash
docker rmi spark-history-server
```